{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "plt.switch_backend('Agg')\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, \n",
    "                             roc_curve, auc, f1_score, precision_score, recall_score, \n",
    "                             roc_auc_score, precision_recall_curve)\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Autres\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from scipy.stats import ks_2samp\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# UMAP (install: pip install umap-learn)\n",
    "try:\n",
    "    import umap.umap_ as umap\n",
    "except ImportError:\n",
    "    print(\"Note: umap-learn not installed. Install with: pip install umap-learn\")\n",
    "\n",
    "### Graphiques inline\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/lucianoleroi/Desktop/projet data/Loan_approval_data_2025.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e879e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_values = (df==0).sum(axis=0)\n",
    "print(no_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb91d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zeero=df.iloc[0]\n",
    "print (df_zeero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d982628",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_values_to_plot = no_values[no_values>0]\n",
    "plt.figure(figsize=(12,6))\n",
    "no_values_to_plot.plot(\n",
    "    kind=\"bar\",\n",
    "    color=\"blue\",\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "plt.title(\n",
    "   \"quantite d'occurences de la valeur 0 pour chaque caracteristique\",\n",
    "   fontsize=16,\n",
    "   fontweight=\"bold\",\n",
    "   pad=20\n",
    "    \n",
    ")\n",
    "\n",
    "for i, v in enumerate (no_values_to_plot):\n",
    "    plt.text(\n",
    "        i,\n",
    "        v + 0.1,\n",
    "        str(v),\n",
    "        color=\"black\",\n",
    "        ha=\"center\",\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=10\n",
    "        \n",
    "    )\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\n--- Résultat du Comptage (Colonnes avec au moins un zéro) ---\")\n",
    "print(no_values_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collone_numerique=df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "collone_categorique= df.select_dtypes(exclude=[np.number]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd336e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for idx , col in enumerate (collone_numerique):\n",
    "    plt.subplot(4,4,idx+1)\n",
    "    sns.histplot(df[col],kde=True, bins=30)\n",
    "    plt.title(col)\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26abe364",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ad0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cor = df[collone_numerique].corr()\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.heatmap(cor, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation avec loan status dans une map de chaleur simple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c867e77",
   "metadata": {},
   "source": [
    "# On peut apercevoir quel seule le credit score  a une correlation significative avec le loan status cor = 0.5 , par ailleurs l'age a une legere incidence cor=0.31.\n",
    "# Par consequent une analise plus approfondie doit etre etablie et on cherche les correlations chachees avec des algoritmes de reduction de dimentionnalite et des differentes matrices de correlation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13693e8",
   "metadata": {},
   "source": [
    "#  SECTION 0: CONTEXTE RÉGLEMENTAIRE ET ÉTHIQUE\n",
    "\n",
    "##  Objectif du Projet\n",
    "\n",
    "Ce projet vise à développer un **modèle de credit scoring** pour prédire l'approbation de prêts bancaires. Il s'agit d'un système d'aide à la décision qui doit respecter des contraintes réglementaires strictes.\n",
    "\n",
    "---\n",
    "\n",
    "##  Cadre Réglementaire Applicable\n",
    "\n",
    "### 1. RGPD (Règlement Général sur la Protection des Données)\n",
    "\n",
    "**Article 22 - Droit à l'explication**\n",
    "- Les décisions automatisées ayant un impact significatif doivent être explicables\n",
    "- Le client a le droit de connaître les raisons d'un refus de prêt\n",
    "- L'opérateur bancaire doit pouvoir justifier chaque décision\n",
    "\n",
    "**Implications pour ce projet:**\n",
    "-  Nous utiliserons SHAP/LIME pour expliquer chaque prédiction\n",
    "-  Un scorecard lisible sera créé pour transparence\n",
    "-  Les coefficients du modèle seront interprétables\n",
    "\n",
    "### 2. Fair Lending Act (Égalité de Traitement)\n",
    "\n",
    "**Principe de non-discrimination**\n",
    "- Interdiction de discriminer sur la base de caractéristiques protégées\n",
    "- Variables protégées: âge, genre, race, origine, religion, statut marital\n",
    "\n",
    "**Implications pour ce projet:**\n",
    "-  Exclusion des variables protégées du modèle\n",
    "-  Tests statistiques de disparate impact (Four-Fifths Rule)\n",
    "-  Analyse de corrélation pour éviter discrimination indirecte\n",
    "\n",
    "### 3. Directives BCE (Banque Centrale Européenne)\n",
    "\n",
    "**Exigences de validation des modèles**\n",
    "- Backtesting sur données historiques\n",
    "- Tests de stabilité (PSI - Population Stability Index)\n",
    "- Documentation complète pour audit\n",
    "\n",
    "---\n",
    "\n",
    "##  Variables Protégées - Gestion des Biais\n",
    "\n",
    "### Variables Exclues du Modèle\n",
    "\n",
    "Les variables suivantes sont **INTERDITES** dans le credit scoring:\n",
    "\n",
    "| Variable | Raison | Statut |\n",
    "|----------|--------|--------|\n",
    "| **Age** | Discrimination par l'âge (sauf si légalement justifié) |  À VÉRIFIER |\n",
    "| **Gender** | Discrimination de genre |  Non présent |\n",
    "| **Race/Ethnicity** | Discrimination raciale |  Non présent |\n",
    "| **Religion** | Discrimination religieuse |  Non présent |\n",
    "| **Statut marital** | Discrimination familiale |  Non présent |\n",
    "| **Origine géographique** | Redlining (discrimination géographique) |  À VÉRIFIER |\n",
    "\n",
    "** ATTENTION:** Même si une variable n'est pas directement utilisée, elle peut créer une discrimination **indirecte** si elle est fortement corrélée avec une variable protégée.\n",
    "\n",
    "### Stratégie de Mitigation\n",
    "\n",
    "Dans ce projet, nous allons:\n",
    "\n",
    "1. **Identifier** les variables potentiellement problématiques (ex: `age`)\n",
    "2. **Analyser** les corrélations avec les variables protégées\n",
    "3. **Tester** le disparate impact sur différents groupes\n",
    "4. **Documenter** les choix d'inclusion/exclusion de variables\n",
    "\n",
    "---\n",
    "\n",
    "##  Définition du Problème Business\n",
    "\n",
    "### Objectif Métier\n",
    "\n",
    "**Maximiser le profit** tout en **minimisant le risque de crédit**, dans le respect des contraintes réglementaires.\n",
    "\n",
    "### Définition du \"Défaut\"\n",
    "\n",
    "Dans ce dataset:\n",
    "- `loan_status = 0` : Prêt **refusé** (client non approuvé)\n",
    "- `loan_status = 1` : Prêt **approuvé** (client accepté)\n",
    "\n",
    "**Note:** Dans un contexte réel de credit scoring, le \"défaut\" serait défini comme:\n",
    "- Retard de paiement > 90 jours\n",
    "- Défaut de paiement total\n",
    "- Restructuration de dette\n",
    "\n",
    "### Coûts Business\n",
    "\n",
    "Les erreurs du modèle ont des impacts financiers asymétriques:\n",
    "\n",
    "| Type d'Erreur | Impact Business | Coût Estimé |\n",
    "|---------------|-----------------|-------------|\n",
    "| **Faux Positif (FP)** | Prêt approuvé qui fera défaut | 15 000 € (perte capitale) |\n",
    "| **Faux Négatif (FN)** | Bon client refusé | 800 € (manque à gagner intérêts) |\n",
    "| **Vrai Positif (TP)** | Bon prêt approuvé | +2 000 € (intérêts) |\n",
    "| **Vrai Négatif (TN)** | Mauvais client correctement refusé | 0 € (évite perte) |\n",
    "\n",
    "**Ratio FP/FN:** Le coût d'un FP est ~19x plus élevé qu'un FN\n",
    "\n",
    " **Implication:** Le seuil de décision doit être optimisé pour minimiser le coût total, pas seulement maximiser l'accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "##  Métriques de Succès\n",
    "\n",
    "### Métriques Techniques (ML)\n",
    "\n",
    "1. **Gini Coefficient** (standard bancaire): Target > 0.40\n",
    "2. **KS Statistic** (Kolmogorov-Smirnov): Target > 0.30\n",
    "3. **AUC-ROC**: Target > 0.75\n",
    "4. **Precision/Recall** équilibrés selon coût FP/FN\n",
    "\n",
    "### Métriques Business\n",
    "\n",
    "1. **Expected Profit** par décision\n",
    "2. **Réduction du taux de défaut** vs règles manuelles\n",
    "3. **Taux d'approbation** (maintenir liquidité)\n",
    "\n",
    "### Métriques Réglementaires\n",
    "\n",
    "1. **Disparate Impact Ratio** > 0.80 (Four-Fifths Rule)\n",
    "2. **PSI** (Population Stability Index) < 0.25\n",
    "3. **Calibration** (Hosmer-Lemeshow p-value > 0.05)\n",
    "\n",
    "---\n",
    "\n",
    "##  Documentation pour Audit\n",
    "\n",
    "Ce notebook constitue la documentation technique du modèle. Il comprend:\n",
    "\n",
    "-  **Justification** de chaque choix méthodologique\n",
    "-  **Traçabilité** des transformations de données\n",
    "-  **Tests** de validation et robustesse\n",
    "-  **Limitations** identifiées\n",
    "-  **Conclusions** scientifiques à chaque étape\n",
    "\n",
    "---\n",
    "\n",
    "##  Limitations et Avertissements\n",
    "\n",
    "### Limitations du Dataset\n",
    "\n",
    "1. **Données synthétiques**: Ce dataset est à visée pédagogique\n",
    "2. **Pas de dimension temporelle**: Impossible de faire du backtesting réel\n",
    "3. **Variables limitées**: Un vrai credit scoring utiliserait 100+ features\n",
    "\n",
    "### Limitations du Modèle\n",
    "\n",
    "1. **Régression logistique simple**: Modèle linéaire, peut manquer interactions complexes\n",
    "2. **Pas de variables externes**: Taux d'intérêt, indicateurs macro-économiques absents\n",
    "3. **Pas de données comportementales**: Historique bancaire, découverts, etc.\n",
    "\n",
    "### Recommandations pour Production\n",
    "\n",
    "Pour un déploiement réel:\n",
    "\n",
    "1.  Tester XGBoost/LightGBM (modèles non-linéaires)\n",
    "2.  Ajouter features comportementales (transactions, soldes)\n",
    "3.  Implémenter monitoring en temps réel (PSI, CSI)\n",
    "4.  A/B testing sur portefeuille réel\n",
    "5.  Validation par département risque + compliance\n",
    "\n",
    "---\n",
    "\n",
    "** Note:** Cette section sera référencée tout au long du notebook pour justifier nos choix méthodologiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Config ----------\n",
    "RANDOM_STATE = 42\n",
    "OUTPUT_DIR = \"./pca_umap_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- hoix de colonnes ----------\n",
    "id_col = \"customer_id\"\n",
    "target_col = \"loan_status\"\n",
    "\n",
    "# Exclusion PCA/UMAP \n",
    "ratios_to_exclude = [\"debt_to_income_ratio\", \"loan_to_income_ratio\", \"payment_to_income_ratio\"]\n",
    "\n",
    "# Ce qu'on ne veut pas sur PCA/UMAP (defaults_on_file y derogatory_marks -> 2.c)\n",
    "exclude_from_embedding = [\"defaults_on_file\", \"derogatory_marks\"]\n",
    "\n",
    "# Categories a One-Hot \n",
    "cat_cols = [\"occupation_status\", \"product_type\", \"loan_intent\"]\n",
    "\n",
    "# Variables numeriques\n",
    "numeric_cols = [\n",
    "    \"age\", \"years_employed\", \"annual_income\", \"credit_score\", \"credit_history_years\",\n",
    "    \"savings_assets\", \"current_debt\", \"loan_amount\", \"interest_rate\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ccb25",
   "metadata": {},
   "source": [
    "# Analyse de Réduction de Dimensionnalité et Séparabilité des Classes\n",
    "\n",
    "## 1. Analyse par Composantes Principales (PCA)\n",
    "\n",
    "L'Analyse par Composantes Principales (PCA) a été appliquée aux données pour identifier les directions de variance maximale et visualiser la séparabilité de la variable cible `loan_status`.\n",
    "\n",
    "### Distribution de la Variance (Scree Plot)\n",
    "\n",
    "L'examen du Scree Plot révèle une très forte concentration de l'information dans la première composante. La **PC1** explique environ **85%** de la variance totale des données. Ceci indique une forte compressibilité du jeu de données dans une dimension principale unique. Les composantes suivantes (PC2, PC3, etc.) contribuent très peu à la variance expliquée (moins de 5% pour PC2).\n",
    "\n",
    "### Projections 2D et 3D\n",
    "\n",
    "Les projections des données sur les composantes principales (PC1 vs PC2, PC1 vs PC3, et la projection 3D) montrent un mélange extrême et un chevauchement presque total des deux classes de `loan_status` (0 et 1). Aucun regroupement spatial des points de même couleur n'est apparent. La PCA conclut que les caractéristiques qui expliquent la plus grande variance dans les données ne sont pas les mêmes qui permettent de distinguer le statut du prêt.\n",
    "\n",
    "## 2. Analyse UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "Afin d'explorer la structure non-linéaire des données, la méthode UMAP a été utilisée.\n",
    "\n",
    "### Structures et Mélange\n",
    "\n",
    "Les visualisations UMAP 2D et 3D révèlent des structures de données complexes et ramifiées(en forme d'étoiles). Cependant, l'observation des points colorés par `loan_status` confirme les conclusions de la PCA. Les deux classes sont intimement mélangées à travers toutes les structures révélées par UMAP. Il n'existe pas de clusters significatifs composés majoritairement d'une seule classe.\n",
    "\n",
    "## 3. Conclusion sur la Séparabilité\n",
    "\n",
    "L'ensemble des analyses de réduction de dimensionnalité (PCA et UMAP) mène à la conclusion que les deux classes de la variable cible `loan_status` sont **indiscernables** sur la base des caractéristiques d'entrée. Ce manque de séparabilité visuelle suggère que l'atteinte d'une grande précision dans un modèle de classification prédictive sera intrinsèquement difficile avec le jeu de caractéristiques actuel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4fca3e",
   "metadata": {},
   "source": [
    "## Constatations sur la Structure des Données\n",
    "\n",
    "L'analyse de réduction de dimensionnalité, combinant la lecture des graphiques de projection et de la matrice de chargement des composantes principales, révèle une structure de données hautement déséquilibrée et inefficace pour la tâche de classification. Le Scree Plot démontre que la **Composante Principale 1 (PC1) capture près de 85% de la variance totale**, indiquant une forte unidimensionalité des données. Cependant, l'examen des coefficients de chargement montre que cette PC1 est presque exclusivement dominée par la variable **`savings_assets`** (coefficient $\\approx 0.998$). La projection des données sur les espaces PCA et UMAP, colorée par la variable cible `loan_status`, montre un **chevauchement intégral** des deux classes. Il est donc établi que la variable expliquant la majorité de la variabilité des données (`savings_assets`) n'est pas informative pour discriminer le statut du prêt, ce qui se traduit par une séparabilité des classes pratiquement nulle dans l'espace des caractéristiques et pose un défi fondamental pour tout modèle de prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c09586",
   "metadata": {},
   "source": [
    "## Changement de dirrection du projet\n",
    "\n",
    "Au lieu de laisser que la PCA maximise la variance, on va forcer le separement des classes par une analyse discriminante linéaire, et la creation de nouvelles variables afin d'appliquer du **featuree engineering* , on creera par la suite des ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a2ihdhcb",
   "metadata": {},
   "source": [
    "#  RÉORIENTATION DE L'ANALYSE: RECHERCHE DE LA SÉPARABILITÉ\n",
    "\n",
    "## Objectif\n",
    "Au lieu de maximiser la variance (PCA), nous cherchons maintenant à **maximiser la séparabilité des classes** pour prédire `loan_status`.\n",
    "\n",
    "---\n",
    "\n",
    "# ÉTAPE A: ANALYSE DISCRIMINANTE LINÉAIRE (LDA)\n",
    "\n",
    "## A1. Préparation et Standardisation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y0yomi83d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger les données transformées (depuis la cellule précédente)\n",
    "# On va utiliser X_scaled_df qui a déjà les variables encodées et standardisées\n",
    "OUTPUT_DIR = \"./pca_umap_outputs\"\n",
    "\n",
    "# Charger X_scaled et y\n",
    "X_scaled_df = pd.read_parquet(os.path.join(OUTPUT_DIR, \"X_scaled.parquet\"), engine=\"fastparquet\")\n",
    "print(f\" Features chargées: {X_scaled_df.shape}\")\n",
    "\n",
    "# Récupérer y depuis df original\n",
    "df = pd.read_csv(\"/Users/lucianoleroi/Desktop/projet data/Loan_approval_data_2025.csv\")\n",
    "target_col = \"loan_status\"\n",
    "\n",
    "# Préparer les données comme dans la cellule PCA/UMAP\n",
    "cat_cols = [\"occupation_status\", \"product_type\", \"loan_intent\"]\n",
    "numeric_cols = [\n",
    "    \"age\", \"years_employed\", \"annual_income\", \"credit_score\", \"credit_history_years\",\n",
    "    \"savings_assets\", \"current_debt\", \"loan_amount\", \"interest_rate\"\n",
    "]\n",
    "\n",
    "# Définir X (DataFrame avec noms de colonnes) et X_standardized (array numpy) pour la LDA\n",
    "X = X_scaled_df  # DataFrame avec noms de colonnes\n",
    "X_standardized = X_scaled_df.values  # Array numpy pour LDA\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"\\n X shape: {X.shape}\")\n",
    "print(f\" X_standardized shape: {X_standardized.shape}\")\n",
    "print(f\" y shape: {y.shape}\")\n",
    "print(f\" Distribution de y: {y.value_counts().to_dict()}\")\n",
    "print(f\"\\n Colonnes disponibles: {X.columns.tolist()[:10]}... (total: {len(X.columns)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kplcljg8mmp",
   "metadata": {},
   "source": [
    "## A2. Application de la LDA (n_components=1)\n",
    "\n",
    "Pour 2 classes, nous avons **k-1 = 1** axe discriminant optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1p86vc7aqhhh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ÉTAPE A2: APPLICATION DE LA LDA ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ÉTAPE A2: APPLICATION DE L'ANALYSE DISCRIMINANTE LINÉAIRE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialiser LDA avec 1 composante (k-1 = 2-1 = 1)\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "\n",
    "# Entraîner et transformer\n",
    "X_lda = lda.fit_transform(X_standardized, y)\n",
    "\n",
    "print(f\" LDA entraînée avec succès\")\n",
    "print(f\" Projection LDA shape: {X_lda.shape}\")\n",
    "print(f\"\\nExplained variance ratio: {lda.explained_variance_ratio_}\")\n",
    "print(f\"Variance totale expliquée: {lda.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "\n",
    "# Créer DataFrame avec la projection LDA\n",
    "lda_df = pd.DataFrame(X_lda, columns=[\"LD1\"])\n",
    "lda_df['loan_status'] = y.values\n",
    "\n",
    "print(f\"\\n DataFrame LDA créé: {lda_df.shape}\")\n",
    "print(\"\\nAperçu des premières lignes:\")\n",
    "print(lda_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r9cnopl48",
   "metadata": {},
   "source": [
    "## A3. Visualisation de la Séparation sur l'Axe Discriminant\n",
    "\n",
    "Histogramme montrant la distribution des deux classes sur l'axe discriminant unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0zw4ghmwky7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ÉTAPE A3: VISUALISATION DE LA SÉPARATION LDA ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ÉTAPE A3: VISUALISATION DE LA SÉPARATION DES CLASSES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Créer figure avec deux sous-graphiques\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- Histogramme avec chevauchement ---\n",
    "ax1 = axes[0]\n",
    "for status in [0, 1]:\n",
    "    subset = lda_df[lda_df['loan_status'] == status]['LD1']\n",
    "    ax1.hist(subset, bins=50, alpha=0.6, label=f'loan_status = {status}', \n",
    "             edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax1.set_xlabel('Axe Discriminant (LD1)', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Fréquence', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Distribution des Classes sur l\\'Axe Discriminant LDA', \n",
    "              fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.legend(fontsize=11, framealpha=0.9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Histogramme empilé pour mieux voir la séparation ---\n",
    "ax2 = axes[1]\n",
    "class_0 = lda_df[lda_df['loan_status'] == 0]['LD1']\n",
    "class_1 = lda_df[lda_df['loan_status'] == 1]['LD1']\n",
    "\n",
    "ax2.hist([class_0, class_1], bins=50, label=['loan_status = 0', 'loan_status = 1'],\n",
    "         color=['#ff7f0e', '#1f77b4'], alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "ax2.set_xlabel('Axe Discriminant (LD1)', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Fréquence', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Distribution Empilée - Axe Discriminant LDA', \n",
    "              fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.legend(fontsize=11, framealpha=0.9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"lda_separation.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistiques de séparation\n",
    "print(\"\\n--- Statistiques de Séparation ---\")\n",
    "for status in [0, 1]:\n",
    "    subset = lda_df[lda_df['loan_status'] == status]['LD1']\n",
    "    print(f\"\\nloan_status = {status}:\")\n",
    "    print(f\"  Moyenne: {subset.mean():.4f}\")\n",
    "    print(f\"  Médiane: {subset.median():.4f}\")\n",
    "    print(f\"  Écart-type: {subset.std():.4f}\")\n",
    "    print(f\"  Min: {subset.min():.4f}, Max: {subset.max():.4f}\")\n",
    "\n",
    "# Calculer la distance entre les moyennes (effet de séparation)\n",
    "mean_0 = lda_df[lda_df['loan_status'] == 0]['LD1'].mean()\n",
    "mean_1 = lda_df[lda_df['loan_status'] == 1]['LD1'].mean()\n",
    "distance = abs(mean_1 - mean_0)\n",
    "\n",
    "print(f\"\\n Distance entre les moyennes des classes: {distance:.4f}\")\n",
    "print(f\"   (Plus la distance est grande, meilleure est la séparation)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" VISUALISATION COMPLÉTÉE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44pgpbtzx6i",
   "metadata": {},
   "source": [
    "## A4. Analyse des Poids/Coefficients Discriminants\n",
    "\n",
    "Les coefficients LDA révèlent **quelles variables contribuent le plus** à la séparation des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2rnklko9m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ÉTAPE A4: ANALYSE DES COEFFICIENTS DISCRIMINANTS ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ÉTAPE A4: ANALYSE DES COEFFICIENTS LDA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extraire les coefficients (poids) de la LDA\n",
    "coefficients = lda.coef_[0]  # Shape: (n_features,)\n",
    "\n",
    "# Créer DataFrame avec les coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': coefficients,\n",
    "    'Abs_Coefficient': np.abs(coefficients)\n",
    "})\n",
    "\n",
    "# Trier par valeur absolue (pouvoir discriminant)\n",
    "coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\n--- TOP 20 FEATURES DISCRIMINANTES ---\")\n",
    "print(coef_df.head(20).to_string(index=False))\n",
    "\n",
    "# Sauvegarder tous les coefficients\n",
    "coef_df.to_csv(os.path.join(OUTPUT_DIR, \"lda_coefficients.csv\"), index=False)\n",
    "print(f\"\\n Tous les coefficients sauvegardés: {OUTPUT_DIR}/lda_coefficients.csv\")\n",
    "\n",
    "# Visualisation des top features\n",
    "top_n = 15\n",
    "top_features = coef_df.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['#d62728' if c < 0 else '#2ca02c' for c in top_features['Coefficient']]\n",
    "bars = plt.barh(range(len(top_features)), top_features['Coefficient'], color=colors, edgecolor='black')\n",
    "\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'], fontsize=10)\n",
    "plt.xlabel('Coefficient Discriminant', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=13, fontweight='bold')\n",
    "plt.title(f'Top {top_n} Features par Pouvoir Discriminant (LDA)', \n",
    "          fontsize=14, fontweight='bold', pad=15)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, (idx, row) in enumerate(top_features.iterrows()):\n",
    "    val = row['Coefficient']\n",
    "    plt.text(val, i, f' {val:.3f}', va='center', \n",
    "             ha='left' if val > 0 else 'right', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"lda_top_coefficients.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistiques des coefficients\n",
    "print(\"\\n--- STATISTIQUES DES COEFFICIENTS ---\")\n",
    "print(f\"Coefficient max (absolu): {coef_df['Abs_Coefficient'].max():.4f}\")\n",
    "print(f\"Coefficient min (absolu): {coef_df['Abs_Coefficient'].min():.4f}\")\n",
    "print(f\"Moyenne (absolu): {coef_df['Abs_Coefficient'].mean():.4f}\")\n",
    "print(f\"Médiane (absolu): {coef_df['Abs_Coefficient'].median():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00qvd3yv9ozu",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ÉTAPE B: FEATURE ENGINEERING (INGÉNIERIE DE CARACTÉRISTIQUES)\n",
    "\n",
    "Création de **ratios financiers** pour transformer les montants bruts en métriques de risque relatives.\n",
    "\n",
    "## B1. Création du Ratio Dette/Revenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6n3503cx3p9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ÉTAPE B1: CRÉATION DU RATIO DETTE/REVENU ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ÉTAPE B1: FEATURE ENGINEERING - RATIO DETTE/REVENU\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Charger les données originales pour créer les nouveaux ratios\n",
    "df_features = df.copy()\n",
    "\n",
    "# B1: Ratio Dette/Revenu = current_debt / annual_income\n",
    "# Gestion des divisions par zéro\n",
    "print(\"\\nCréation du ratio dette/revenu...\")\n",
    "df_features['ratio_dette_revenu'] = np.where(\n",
    "    df_features['annual_income'] > 0,\n",
    "    df_features['current_debt'] / df_features['annual_income'],\n",
    "    0  # Si revenu = 0, ratio = 0\n",
    ")\n",
    "\n",
    "print(f\" Ratio dette/revenu créé\")\n",
    "print(f\"\\nStatistiques du ratio dette/revenu:\")\n",
    "print(df_features['ratio_dette_revenu'].describe())\n",
    "\n",
    "# Vérifier les valeurs infinies ou NaN\n",
    "n_inf = np.isinf(df_features['ratio_dette_revenu']).sum()\n",
    "n_nan = df_features['ratio_dette_revenu'].isna().sum()\n",
    "print(f\"\\nValeurs infinies: {n_inf}\")\n",
    "print(f\"Valeurs NaN: {n_nan}\")\n",
    "\n",
    "# Visualisation de la distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogramme\n",
    "ax1 = axes[0]\n",
    "ax1.hist(df_features['ratio_dette_revenu'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Ratio Dette/Revenu', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Fréquence', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution du Ratio Dette/Revenu', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Boxplot par classe\n",
    "ax2 = axes[1]\n",
    "df_features.boxplot(column='ratio_dette_revenu', by='loan_status', ax=ax2)\n",
    "ax2.set_xlabel('Loan Status', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Ratio Dette/Revenu', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Ratio Dette/Revenu par Classe', fontsize=13, fontweight='bold')\n",
    "plt.suptitle('')  # Supprimer le titre automatique\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"ratio_dette_revenu.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" RATIO DETTE/REVENU CRÉÉ AVEC SUCCÈS\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ssaggb2scla",
   "metadata": {},
   "source": [
    "## B2. Création du Ratio Prêt/Actifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lca4tdcswxf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ÉTAPE B2: CRÉATION DU RATIO PRÊT/ACTIFS ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ÉTAPE B2: FEATURE ENGINEERING - RATIO PRÊT/ACTIFS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# B2: Ratio Prêt/Actifs = loan_amount / savings_assets\n",
    "# Gestion des divisions par zéro\n",
    "print(\"\\nCréation du ratio prêt/actifs...\")\n",
    "df_features['ratio_pret_actifs'] = np.where(\n",
    "    df_features['savings_assets'] > 0,\n",
    "    df_features['loan_amount'] / df_features['savings_assets'],\n",
    "    0  # Si actifs = 0, ratio = 0 (ou on pourrait utiliser np.inf selon le cas)\n",
    ")\n",
    "\n",
    "print(f\" Ratio prêt/actifs créé\")\n",
    "print(f\"\\nStatistiques du ratio prêt/actifs:\")\n",
    "print(df_features['ratio_pret_actifs'].describe())\n",
    "\n",
    "# Vérifier les valeurs infinies ou NaN\n",
    "n_inf = np.isinf(df_features['ratio_pret_actifs']).sum()\n",
    "n_nan = df_features['ratio_pret_actifs'].isna().sum()\n",
    "print(f\"\\nValeurs infinies: {n_inf}\")\n",
    "print(f\"Valeurs NaN: {n_nan}\")\n",
    "\n",
    "# Gestion des outliers extrêmes pour la visualisation\n",
    "# On va clipper les valeurs au 99ème percentile pour mieux voir\n",
    "p99 = df_features['ratio_pret_actifs'].quantile(0.99)\n",
    "df_features['ratio_pret_actifs_clipped'] = df_features['ratio_pret_actifs'].clip(upper=p99)\n",
    "\n",
    "print(f\"\\nValeur du 99ème percentile: {p99:.2f}\")\n",
    "print(f\"Nombre de valeurs au-dessus: {(df_features['ratio_pret_actifs'] > p99).sum()}\")\n",
    "\n",
    "# Visualisation de la distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogramme (avec clipping pour meilleure visualisation)\n",
    "ax1 = axes[0]\n",
    "ax1.hist(df_features['ratio_pret_actifs_clipped'], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "ax1.set_xlabel('Ratio Prêt/Actifs (clipped à p99)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Fréquence', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution du Ratio Prêt/Actifs', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Boxplot par classe (avec clipping)\n",
    "ax2 = axes[1]\n",
    "df_features.boxplot(column='ratio_pret_actifs_clipped', by='loan_status', ax=ax2)\n",
    "ax2.set_xlabel('Loan Status', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Ratio Prêt/Actifs (clipped à p99)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Ratio Prêt/Actifs par Classe', fontsize=13, fontweight='bold')\n",
    "plt.suptitle('')  # Supprimer le titre automatique\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"ratio_pret_actifs.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" RATIO PRÊT/ACTIFS CRÉÉ AVEC SUCCÈS\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ww3ks2taq7l",
   "metadata": {},
   "source": [
    "## B3. Visualisation Croisée des Nouveaux Ratios\n",
    "\n",
    "Analyse de la séparation des classes dans l'espace 2D des ratios créés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlmjkjvd40s",
   "metadata": {},
   "source": [
    "## B4. Création de Features d'Interaction\n",
    "\n",
    "Exploration de patterns cachés par création de features d'interaction entre variables clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0jc2ld0tt5ut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ETAPE B4: CREATION DE FEATURES D'INTERACTION ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ETAPE B4: FEATURE ENGINEERING - FEATURES D'INTERACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Créer des features d'interaction basées sur les insights LDA\n",
    "print(\"\\nCreation de features d'interaction...\")\n",
    "\n",
    "# 1. Income x Credit Score (capacité financière globale)\n",
    "df_features['income_credit_interaction'] = df_features['annual_income'] * df_features['credit_score']\n",
    "\n",
    "# 2. Debt x Interest Rate (coût total du crédit)\n",
    "df_features['debt_interest_interaction'] = df_features['current_debt'] * df_features['interest_rate']\n",
    "\n",
    "# 3. Loan Amount x Interest Rate (coût du prêt)\n",
    "df_features['loan_interest_interaction'] = df_features['loan_amount'] * df_features['interest_rate']\n",
    "\n",
    "# 4. Income x Credit History (stabilité financière)\n",
    "df_features['income_history_interaction'] = df_features['annual_income'] * df_features['credit_history_years']\n",
    "\n",
    "# 5. Savings x Credit Score (solvabilité)\n",
    "df_features['savings_credit_interaction'] = df_features['savings_assets'] * df_features['credit_score']\n",
    "\n",
    "# 6. Age x Years Employed (expérience professionnelle)\n",
    "df_features['age_employed_interaction'] = df_features['age'] * df_features['years_employed']\n",
    "\n",
    "# 7. Ratio Debt to Income x Loan Amount (risque ajusté)\n",
    "df_features['risk_adjusted_loan'] = df_features['ratio_dette_revenu'] * df_features['loan_amount']\n",
    "\n",
    "# 8. Credit Score / Debt (capacité de remboursement)\n",
    "df_features['credit_debt_ratio'] = np.where(\n",
    "    df_features['current_debt'] > 0,\n",
    "    df_features['credit_score'] / df_features['current_debt'],\n",
    "    df_features['credit_score']  # Si pas de dette, utiliser le credit score\n",
    ")\n",
    "\n",
    "print(\"Features d'interaction creees:\")\n",
    "interaction_features = [\n",
    "    'income_credit_interaction',\n",
    "    'debt_interest_interaction', \n",
    "    'loan_interest_interaction',\n",
    "    'income_history_interaction',\n",
    "    'savings_credit_interaction',\n",
    "    'age_employed_interaction',\n",
    "    'risk_adjusted_loan',\n",
    "    'credit_debt_ratio'\n",
    "]\n",
    "\n",
    "for feat in interaction_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "# Calculer les corrélations avec loan_status\n",
    "print(\"\\n--- CORRELATIONS AVEC LOAN_STATUS ---\")\n",
    "correlations = {}\n",
    "for feat in interaction_features:\n",
    "    if feat in df_features.columns:\n",
    "        corr = df_features[feat].corr(df_features['loan_status'])\n",
    "        correlations[feat] = corr\n",
    "        print(f\"  {feat:35s}: {corr:+.4f}\")\n",
    "\n",
    "# Trier par corrélation absolue\n",
    "sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\n--- TOP 5 FEATURES D'INTERACTION (par correlation) ---\")\n",
    "for feat, corr in sorted_corr[:5]:\n",
    "    impact = \"POSITIF\" if corr > 0 else \"NEGATIF\"\n",
    "    strength = \"FORT\" if abs(corr) > 0.3 else \"MOYEN\" if abs(corr) > 0.1 else \"FAIBLE\"\n",
    "    print(f\"  {feat:35s}: {corr:+.4f} (Impact {impact} {strength})\")\n",
    "\n",
    "# Visualisation des distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Top 4 features d'interaction\n",
    "top_features = [f for f, c in sorted_corr[:4]]\n",
    "\n",
    "for idx, feat in enumerate(top_features):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Boxplot par classe\n",
    "    df_features.boxplot(column=feat, by='loan_status', ax=ax)\n",
    "    ax.set_xlabel('Loan Status', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel(feat, fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{feat}\\n(Corr: {correlations[feat]:+.3f})', fontsize=12, fontweight='bold')\n",
    "    plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"features_interaction.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60mogmdpw4j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ÉTAPE B3: VISUALISATION CROISÉE DES RATIOS ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ÉTAPE B3: VISUALISATION CROISÉE DES NOUVEAUX RATIOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Préparer les données pour la visualisation (supprimer les NaN)\n",
    "df_viz = df_features[['ratio_dette_revenu', 'ratio_pret_actifs', 'loan_status']].copy()\n",
    "df_viz = df_viz.dropna()\n",
    "\n",
    "# Clipper les valeurs extrêmes pour meilleure visualisation\n",
    "p99_dette = df_viz['ratio_dette_revenu'].quantile(0.99)\n",
    "p99_pret = df_viz['ratio_pret_actifs'].quantile(0.99)\n",
    "\n",
    "df_viz['ratio_dette_revenu_viz'] = df_viz['ratio_dette_revenu'].clip(upper=p99_dette)\n",
    "df_viz['ratio_pret_actifs_viz'] = df_viz['ratio_pret_actifs'].clip(upper=p99_pret)\n",
    "\n",
    "print(f\" Données préparées pour visualisation: {df_viz.shape[0]:,} lignes\")\n",
    "print(f\"  Clipping dette/revenu à: {p99_dette:.2f}\")\n",
    "print(f\"  Clipping prêt/actifs à: {p99_pret:.2f}\")\n",
    "\n",
    "# Créer la figure avec plusieurs vues\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# --- Vue 1: Scatter plot simple ---\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "colors = ['#ff7f0e', '#1f77b4']\n",
    "for status in [0, 1]:\n",
    "    subset = df_viz[df_viz['loan_status'] == status]\n",
    "    ax1.scatter(subset['ratio_dette_revenu_viz'], subset['ratio_pret_actifs_viz'],\n",
    "               alpha=0.3, s=8, c=colors[status], label=f'loan_status = {status}',\n",
    "               edgecolors='none')\n",
    "\n",
    "ax1.set_xlabel('Ratio Dette/Revenu', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Ratio Prêt/Actifs', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Scatter Plot: Ratios par Classe', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Vue 2: Density plot (hexbin) pour classe 0 ---\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "subset_0 = df_viz[df_viz['loan_status'] == 0]\n",
    "hexbin = ax2.hexbin(subset_0['ratio_dette_revenu_viz'], subset_0['ratio_pret_actifs_viz'],\n",
    "                    gridsize=30, cmap='Oranges', alpha=0.8, edgecolors='black', linewidths=0.2)\n",
    "ax2.set_xlabel('Ratio Dette/Revenu', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Ratio Prêt/Actifs', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Densité: loan_status = 0', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(hexbin, ax=ax2, label='Nombre d\\'observations')\n",
    "\n",
    "# --- Vue 3: Density plot (hexbin) pour classe 1 ---\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "subset_1 = df_viz[df_viz['loan_status'] == 1]\n",
    "hexbin = ax3.hexbin(subset_1['ratio_dette_revenu_viz'], subset_1['ratio_pret_actifs_viz'],\n",
    "                    gridsize=30, cmap='Blues', alpha=0.8, edgecolors='black', linewidths=0.2)\n",
    "ax3.set_xlabel('Ratio Dette/Revenu', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Ratio Prêt/Actifs', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Densité: loan_status = 1', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(hexbin, ax=ax3, label='Nombre d\\'observations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"ratios_croises_separation.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistiques comparatives\n",
    "print(\"\\n--- STATISTIQUES COMPARATIVES PAR CLASSE ---\")\n",
    "for status in [0, 1]:\n",
    "    subset = df_viz[df_viz['loan_status'] == status]\n",
    "    print(f\"\\nloan_status = {status} (n={len(subset):,}):\")\n",
    "    print(f\"  Ratio dette/revenu - moyenne: {subset['ratio_dette_revenu'].mean():.4f}, médiane: {subset['ratio_dette_revenu'].median():.4f}\")\n",
    "    print(f\"  Ratio prêt/actifs - moyenne: {subset['ratio_pret_actifs'].mean():.4f}, médiane: {subset['ratio_pret_actifs'].median():.4f}\")\n",
    "\n",
    "# Corrélation des nouveaux ratios avec loan_status\n",
    "\n",
    "corr_dette = df_viz['ratio_dette_revenu'].corr(df_viz['loan_status'])\n",
    "corr_pret = df_viz['ratio_pret_actifs'].corr(df_viz['loan_status'])\n",
    "print(f\"Ratio dette/revenu vs loan_status: {corr_dette:.4f}\")\n",
    "print(f\"Ratio prêt/actifs vs loan_status: {corr_pret:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bvji55p56jq",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ÉTAPE C: MODÉLISATION DE BASE (BASELINE MODEL)\n",
    "\n",
    "Création d'un modèle de référence avec gestion du déséquilibre et évaluation rigoureuse.\n",
    "\n",
    "## C1. Gestion du Déséquilibre avec SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9qy7zyidcb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ETAPE C (VERSION AMELIOREE): MODELISATION AVANCEE\n",
    "\n",
    "Approche améliorée avec:\n",
    "- KNN Imputer pour gérer les valeurs manquantes (préservation des distributions)\n",
    "- Features d'interaction incluses\n",
    "- Pas de SMOTE (déséquilibre naturel conservé)\n",
    "- class_weight='balanced' dans le modèle\n",
    "\n",
    "## C1. Préparation des Données avec KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9yc28vz72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ÉTAPE C1: GESTION DU DÉSÉQUILIBRE AVEC SMOTE ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ÉTAPE C1: PRÉPARATION DES DONNÉES + SMOTE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Préparer le dataset complet avec les NOUVEAUX RATIOS\n",
    "print(\"\\nPréparation du dataset avec features originales + nouveaux ratios...\")\n",
    "\n",
    "# Features numériques originales\n",
    "numeric_features = [\n",
    "    \"age\", \"years_employed\", \"annual_income\", \"credit_score\", \"credit_history_years\",\n",
    "    \"savings_assets\", \"current_debt\", \"loan_amount\", \"interest_rate\"\n",
    "]\n",
    "\n",
    "# Features catégorielles\n",
    "cat_features = [\"occupation_status\", \"product_type\", \"loan_intent\"]\n",
    "\n",
    "# Nouveaux ratios\n",
    "new_ratios = [\"ratio_dette_revenu\", \"ratio_pret_actifs\"]\n",
    "\n",
    "# Sélectionner les colonnes\n",
    "cols_to_use = numeric_features + cat_features + new_ratios + ['loan_status']\n",
    "df_c = df_features[cols_to_use].copy()\n",
    "\n",
    "print(f\" Dataset initial: {df_c.shape}\")\n",
    "\n",
    "# Supprimer les NaN\n",
    "df_c = df_c.dropna()\n",
    "print(f\" Après suppression des NaN: {df_c.shape}\")\n",
    "\n",
    "# One-hot encoding des variables catégorielles\n",
    "print(\"\\nOne-hot encoding des variables catégorielles...\")\n",
    "df_c_encoded = pd.get_dummies(df_c, columns=cat_features, prefix_sep=\"__\", drop_first=False)\n",
    "\n",
    "# Séparer X et y\n",
    "X = df_c_encoded.drop(columns=['loan_status'])\n",
    "y = df_c_encoded['loan_status'].astype(int)\n",
    "\n",
    "print(f\" Features après encoding: {X.shape}\")\n",
    "print(f\" Distribution de y: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Train/test split\n",
    "print(\"\\nTrain/test split (80/20, stratifié)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\" X_train: {X_train.shape}\")\n",
    "print(f\" X_test: {X_test.shape}\")\n",
    "print(f\" y_train distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\" y_test distribution: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Standardisation\n",
    "print(\"\\nStandardisation des features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\" X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\" X_test_scaled: {X_test_scaled.shape}\")\n",
    "\n",
    "# Application de SMOTE sur le training set seulement\n",
    "print(\"\\nApplication de SMOTE pour rééquilibrer les classes...\")\n",
    "print(f\"  Distribution AVANT SMOTE: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"  Distribution APRÈS SMOTE: {dict(zip(*np.unique(y_train_resampled, return_counts=True)))}\")\n",
    "print(f\" X_train_resampled: {X_train_resampled.shape}\")\n",
    "print(f\" y_train_resampled: {y_train_resampled.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" DONNÉES PRÉPARÉES ET RÉÉQUILIBRÉES\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jzqp99ae3z",
   "metadata": {},
   "source": [
    "## C2. Entraînement du Modèle de Référence (Régression Logistique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qmkmd5upzg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ÉTAPE C2: ENTRAÎNEMENT DU MODÈLE DE RÉFÉRENCE ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ÉTAPE C2: ENTRAÎNEMENT DU MODÈLE DE RÉFÉRENCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialiser le modèle de régression logistique\n",
    "print(\"\\nInitialisation de la Régression Logistique...\")\n",
    "log_reg = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,  # Augmenter pour assurer la convergence\n",
    "    solver='lbfgs',  # Bon pour datasets moyens/grands\n",
    "    class_weight='balanced'  # Poids équilibrés (alternative à SMOTE, on garde les deux)\n",
    ")\n",
    "\n",
    "# Entraîner sur les données SMOTE-résamplées\n",
    "print(\"Entraînement du modèle sur les données SMOTE...\")\n",
    "log_reg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(f\" Modèle entraîné avec succès\")\n",
    "\n",
    "# Prédictions sur le jeu de test (ATTENTION: on teste sur les données NON-resamplées)\n",
    "print(\"\\nPrédictions sur le jeu de test...\")\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probabilités classe 1\n",
    "\n",
    "print(f\" Prédictions complétées\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2uj9x3j75u4",
   "metadata": {},
   "source": [
    "## C3. Évaluation avec Métriques Ciblées\n",
    "\n",
    "Évaluation rigoureuse du modèle avec les métriques appropriées pour les classes déséquilibrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mwidpbffnur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETAPE C3: EVALUATION AVEC METRIQUES CIBLEES\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ETAPE C3: EVALUATION DU MODELE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# METRIQUES PRINCIPALES\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"METRIQUES DE PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# F1-Score (metrique principale pour classes desequilibrees)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"\\nF1-SCORE: {f1:.4f}\")\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Rappel\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f\"Rappel: {recall:.4f}\")\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# RAPPORT DE CLASSIFICATION DETAILLE\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RAPPORT DE CLASSIFICATION DETAILLE\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, y_pred, target_names=['Classe 0 (Refuse)', 'Classe 1 (Approuve)']))\n",
    "\n",
    "# MATRICE DE CONFUSION\n",
    "print(\"=\" * 70)\n",
    "print(\"MATRICE DE CONFUSION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nMatrice de confusion brute:\")\n",
    "print(cm)\n",
    "\n",
    "# Interpretation\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nDetails:\")\n",
    "print(f\"  Vrais Negatifs (TN):  {tn:5d} - Refus correctement predits\")\n",
    "print(f\"  Faux Positifs (FP):   {fp:5d} - Refus predits comme Approuves\")\n",
    "print(f\"  Faux Negatifs (FN):   {fn:5d} - Approuves predits comme Refus\")\n",
    "print(f\"  Vrais Positifs (TP):  {tp:5d} - Approbations correctement predites\")\n",
    "\n",
    "# Visualisation de la matrice de confusion\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Matrice de confusion en valeurs absolues\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax1,\n",
    "            xticklabels=['Refuse (0)', 'Approuve (1)'],\n",
    "            yticklabels=['Refuse (0)', 'Approuve (1)'])\n",
    "ax1.set_xlabel('Predictions', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Valeurs Reelles', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Matrice de Confusion (Valeurs Absolues)', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "# Matrice de confusion normalisee\n",
    "ax2 = axes[1]\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', cbar=True, ax=ax2,\n",
    "            xticklabels=['Refuse (0)', 'Approuve (1)'],\n",
    "            yticklabels=['Refuse (0)', 'Approuve (1)'])\n",
    "ax2.set_xlabel('Predictions', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Valeurs Reelles', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Matrice de Confusion (Normalisee par ligne)', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix_baseline.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# COURBE ROC\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COURBE ROC\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"\\nAUC (Area Under Curve): {roc_auc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Courbe ROC (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Hasard (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de Faux Positifs (FPR)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Taux de Vrais Positifs (TPR)', fontsize=13, fontweight='bold')\n",
    "plt.title('Courbe ROC - Regression Logistique (Baseline)', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"roc_curve_baseline.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# RESUME FINAL\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION COMPLETEE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nRESUME DES PERFORMANCES:\")\n",
    "print(f\"  F1-Score:   {f1:.4f}\")\n",
    "print(f\"  Precision:  {precision:.4f}\")\n",
    "print(f\"  Rappel:     {recall:.4f}\")\n",
    "print(f\"  Accuracy:   {accuracy:.4f}\")\n",
    "print(f\"  AUC-ROC:    {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xxlhsri5etl",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ETAPE D: MODELISATION AMELIOREE\n",
    "\n",
    "Nouvelle approche avec:\n",
    "- KNN Imputer pour les valeurs manquantes\n",
    "- Features d'interaction incluses  \n",
    "- Pas de SMOTE (déséquilibre naturel + class_weight='balanced')\n",
    "\n",
    "## D1. Préparation des Données avec KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hw9vtafr53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ETAPE D1: PREPARATION AVEC KNN IMPUTER ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ETAPE D1: PREPARATION DES DONNEES (VERSION AMELIOREE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Features numériques originales\n",
    "numeric_features = [\n",
    "    \"age\", \"years_employed\", \"annual_income\", \"credit_score\", \"credit_history_years\",\n",
    "    \"savings_assets\", \"current_debt\", \"loan_amount\", \"interest_rate\"\n",
    "]\n",
    "\n",
    "# Nouveaux ratios\n",
    "new_ratios = [\"ratio_dette_revenu\", \"ratio_pret_actifs\"]\n",
    "\n",
    "# Features d'interaction\n",
    "interaction_features = [\n",
    "    'income_credit_interaction',\n",
    "    'debt_interest_interaction', \n",
    "    'loan_interest_interaction',\n",
    "    'income_history_interaction',\n",
    "    'savings_credit_interaction',\n",
    "    'age_employed_interaction',\n",
    "    'risk_adjusted_loan',\n",
    "    'credit_debt_ratio'\n",
    "]\n",
    "\n",
    "# Features catégorielles\n",
    "cat_features = [\"occupation_status\", \"product_type\", \"loan_intent\"]\n",
    "\n",
    "# Toutes les features numériques\n",
    "all_numeric_features = numeric_features + new_ratios + interaction_features\n",
    "\n",
    "# Sélectionner les colonnes\n",
    "cols_to_use = all_numeric_features + cat_features + ['loan_status']\n",
    "df_model_v2 = df_features[cols_to_use].copy()\n",
    "\n",
    "print(f\"\\nDataset initial: {df_model_v2.shape}\")\n",
    "print(f\"Features numeriques: {len(all_numeric_features)}\")\n",
    "print(f\"Features categorielles: {len(cat_features)}\")\n",
    "\n",
    "# Analyser les valeurs manquantes AVANT imputation\n",
    "print(\"\\n--- ANALYSE DES VALEURS MANQUANTES ---\")\n",
    "missing_counts = df_model_v2[all_numeric_features].isnull().sum()\n",
    "missing_cols = missing_counts[missing_counts > 0]\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(\"Colonnes avec valeurs manquantes:\")\n",
    "    for col, count in missing_cols.items():\n",
    "        pct = count / len(df_model_v2) * 100\n",
    "        print(f\"  {col:30s}: {count:5d} ({pct:.2f}%)\")\n",
    "    \n",
    "    # KNN IMPUTATION\n",
    "    print(f\"\\n--- APPLICATION KNN IMPUTER (k=5) ---\")\n",
    "    knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    \n",
    "    df_model_v2[all_numeric_features] = knn_imputer.fit_transform(df_model_v2[all_numeric_features])\n",
    "    \n",
    "    print(\"KNN Imputer applique avec succes\")\n",
    "    print(\"  Methode: k=5 voisins avec ponderation par distance\")\n",
    "    print(\"  Avantage: Preserve les distributions et correlations\")\n",
    "    \n",
    "    remaining_nan = df_model_v2[all_numeric_features].isnull().sum().sum()\n",
    "    print(f\"\\nValeurs manquantes restantes: {remaining_nan}\")\n",
    "else:\n",
    "    print(\"Aucune valeur manquante detectee\")\n",
    "\n",
    "# One-hot encoding\n",
    "print(\"\\n--- ONE-HOT ENCODING ---\")\n",
    "df_model_v2_encoded = pd.get_dummies(df_model_v2, columns=cat_features, prefix_sep=\"__\", drop_first=False)\n",
    "print(f\"Dataset apres encoding: {df_model_v2_encoded.shape}\")\n",
    "\n",
    "# Séparer X et y\n",
    "X_full_v2 = df_model_v2_encoded.drop(columns=['loan_status'])\n",
    "y_full_v2 = df_model_v2_encoded['loan_status'].astype(int)\n",
    "\n",
    "print(f\"\\nX shape: {X_full_v2.shape}\")\n",
    "print(f\"y shape: {y_full_v2.shape}\")\n",
    "\n",
    "# Distribution\n",
    "print(f\"\\n--- DISTRIBUTION DE LOAN_STATUS ---\")\n",
    "print(y_full_v2.value_counts())\n",
    "print(f\"\\nProportions:\")\n",
    "print(y_full_v2.value_counts(normalize=True))\n",
    "\n",
    "# Split Train/Test\n",
    "print(\"\\n--- SPLIT TRAIN/TEST (80/20) ---\")\n",
    "X_train_v2, X_test_v2, y_train_v2, y_test_v2 = train_test_split(\n",
    "    X_full_v2, y_full_v2, test_size=0.2, random_state=42, stratify=y_full_v2\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train_v2.shape}\")\n",
    "print(f\"Test set:  {X_test_v2.shape}\")\n",
    "\n",
    "# Standardisation\n",
    "print(\"\\n--- STANDARDISATION ---\")\n",
    "scaler_v2 = StandardScaler()\n",
    "X_train_v2_scaled = scaler_v2.fit_transform(X_train_v2)\n",
    "X_test_v2_scaled = scaler_v2.transform(X_test_v2)\n",
    "\n",
    "print(\"Standardisation completee\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DONNEES PRETES (PAS DE SMOTE - DESEQUILIBRE NATUREL CONSERVE)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTotal features: {X_train_v2_scaled.shape[1]}\")\n",
    "print(f\"  - Features originales: {len(numeric_features)}\")\n",
    "print(f\"  - Ratios: {len(new_ratios)}\")\n",
    "print(f\"  - Interactions: {len(interaction_features)}\")\n",
    "print(f\"  - Categorielles encodees: {X_train_v2_scaled.shape[1] - len(all_numeric_features)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l1cli601ufc",
   "metadata": {},
   "source": [
    "## D2. Entraînement du Modèle Amélioré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g5w9ezqc6ar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ETAPE D2: ENTRAINEMENT DU MODELE AMELIORE ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ETAPE D2: ENTRAINEMENT DU MODELE AMELIORE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Régression Logistique avec class_weight='balanced'\n",
    "print(\"\\nInitialisation de la Regression Logistique...\")\n",
    "print(\"  Parametres:\")\n",
    "print(\"    - class_weight='balanced' (gestion du desequilibre)\")\n",
    "print(\"    - max_iter=1000\")\n",
    "print(\"    - solver='lbfgs'\")\n",
    "\n",
    "log_reg_v2 = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced'  # Pondération automatique selon les proportions\n",
    ")\n",
    "\n",
    "# Entraînement\n",
    "print(\"\\nEntrainement sur le jeu d'entrainement...\")\n",
    "log_reg_v2.fit(X_train_v2_scaled, y_train_v2)\n",
    "\n",
    "print(\"Modele entraine avec succes\")\n",
    "\n",
    "# Prédictions\n",
    "print(\"\\nPredictions sur le jeu de test...\")\n",
    "y_pred_v2 = log_reg_v2.predict(X_test_v2_scaled)\n",
    "y_pred_proba_v2 = log_reg_v2.predict_proba(X_test_v2_scaled)[:, 1]\n",
    "\n",
    "print(\"Predictions completees\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODELE ENTRAINE ET PREDICTIONS EFFECTUEES\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9qta63sbk9o",
   "metadata": {},
   "source": [
    "## D3. Évaluation et Comparaison des Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01hmldywopq2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ETAPE D3: EVALUATION ET COMPARAISON ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ETAPE D3: EVALUATION DU MODELE AMELIORE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Métriques\n",
    "f1_v2 = f1_score(y_test_v2, y_pred_v2)\n",
    "precision_v2 = precision_score(y_test_v2, y_pred_v2)\n",
    "recall_v2 = recall_score(y_test_v2, y_pred_v2)\n",
    "accuracy_v2 = accuracy_score(y_test_v2, y_pred_v2)\n",
    "\n",
    "print(\"\\n--- METRIQUES DE PERFORMANCE (MODELE AMELIORE) ---\")\n",
    "print(f\"  F1-Score:   {f1_v2:.4f}\")\n",
    "print(f\"  Precision:  {precision_v2:.4f}\")\n",
    "print(f\"  Rappel:     {recall_v2:.4f}\")\n",
    "print(f\"  Accuracy:   {accuracy_v2:.4f}\")\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\n--- RAPPORT DE CLASSIFICATION ---\")\n",
    "print(classification_report(y_test_v2, y_pred_v2, \n",
    "                          target_names=['Classe 0 (Refuse)', 'Classe 1 (Approuve)']))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm_v2 = confusion_matrix(y_test_v2, y_pred_v2)\n",
    "tn_v2, fp_v2, fn_v2, tp_v2 = cm_v2.ravel()\n",
    "\n",
    "print(\"\\n--- MATRICE DE CONFUSION ---\")\n",
    "print(f\"  Vrais Negatifs (TN):  {tn_v2:5,}\")\n",
    "print(f\"  Faux Positifs (FP):   {fp_v2:5,}\")\n",
    "print(f\"  Faux Negatifs (FN):   {fn_v2:5,}\")\n",
    "print(f\"  Vrais Positifs (TP):  {tp_v2:5,}\")\n",
    "\n",
    "# Courbe ROC\n",
    "fpr_v2, tpr_v2, _ = roc_curve(y_test_v2, y_pred_proba_v2)\n",
    "auc_v2 = auc(fpr_v2, tpr_v2)\n",
    "\n",
    "print(f\"\\n  AUC-ROC: {auc_v2:.4f}\")\n",
    "\n",
    "# Visualisation matrice de confusion + courbe ROC\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Matrice de confusion\n",
    "ax1 = axes[0]\n",
    "sns.heatmap(cm_v2, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax1,\n",
    "            xticklabels=['Refuse (0)', 'Approuve (1)'],\n",
    "            yticklabels=['Refuse (0)', 'Approuve (1)'])\n",
    "ax1.set_xlabel('Predictions', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Valeurs Reelles', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('Matrice de Confusion (Modele Ameliore)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Courbe ROC\n",
    "ax2 = axes[1]\n",
    "ax2.plot(fpr_v2, tpr_v2, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_v2:.4f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('Taux de Faux Positifs', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Taux de Vrais Positifs', fontsize=13, fontweight='bold')\n",
    "ax2.set_title('Courbe ROC (Modele Ameliore)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc=\"lower right\", fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"model_v2_evaluation.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# COMPARAISON AVEC LE MODELE BASELINE (C1, C2, C3)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARAISON: BASELINE vs MODELE AMELIORE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Métriques baseline (depuis C3)\n",
    "f1_baseline = 0.8405\n",
    "precision_baseline = 0.8538\n",
    "recall_baseline = 0.8276\n",
    "accuracy_baseline = 0.8271\n",
    "\n",
    "print(\"\\n| Metrique      | Baseline (SMOTE) | Ameliore (KNN+Features) | Difference |\")\n",
    "print(\"|\" + \"-\"*14 + \"|\" + \"-\"*18 + \"|\" + \"-\"*25 + \"|\" + \"-\"*12 + \"|\")\n",
    "print(f\"| F1-Score      | {f1_baseline:16.4f} | {f1_v2:23.4f} | {f1_v2 - f1_baseline:+10.4f} |\")\n",
    "print(f\"| Precision     | {precision_baseline:16.4f} | {precision_v2:23.4f} | {precision_v2 - precision_baseline:+10.4f} |\")\n",
    "print(f\"| Rappel        | {recall_baseline:16.4f} | {recall_v2:23.4f} | {recall_v2 - recall_baseline:+10.4f} |\")\n",
    "print(f\"| Accuracy      | {accuracy_baseline:16.4f} | {accuracy_v2:23.4f} | {accuracy_v2 - accuracy_baseline:+10.4f} |\")\n",
    "\n",
    "# Analyse\n",
    "print(\"\\n--- ANALYSE ---\")\n",
    "if f1_v2 > f1_baseline:\n",
    "    print(f\"  AMELIORATION de {(f1_v2 - f1_baseline)*100:.2f}% du F1-Score\")\n",
    "    print(\"  -> Les features d'interaction et KNN Imputer apportent de la valeur\")\n",
    "elif f1_v2 < f1_baseline:\n",
    "    print(f\"  DEGRADATION de {(f1_baseline - f1_v2)*100:.2f}% du F1-Score\")\n",
    "    print(\"  -> SMOTE etait plus efficace que class_weight='balanced'\")\n",
    "else:\n",
    "    print(\"  Performances EQUIVALENTES\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION TERMINEE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8p7yuxxeqlu",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ETAPE E: MODELISATION COMPARATIVE AVEC VALIDATION CROISEE\n",
    "\n",
    "Comparaison systématique de 4 approches:\n",
    "1. **Baseline + SMOTE** (C2/C3) - features originales\n",
    "2. **Amélioré KNN + Features SANS SMOTE** (D2/D3)\n",
    "3. **Baseline SANS SMOTE** (nouvelle)\n",
    "4. **Amélioré KNN + Features + SMOTE** (nouvelle)\n",
    "\n",
    "Toutes avec validation croisée Stratified K-Fold (k=5).\n",
    "\n",
    "## E1. Baseline SANS SMOTE (class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ljqg1mw3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== E1: BASELINE SANS SMOTE (class_weight='balanced') ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"E1: BASELINE SANS SMOTE (FEATURES ORIGINALES)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Utiliser les données de C1 (features originales sans features d'interaction)\n",
    "# Recréer le dataset baseline\n",
    "numeric_features_base = [\n",
    "    \"age\", \"years_employed\", \"annual_income\", \"credit_score\", \"credit_history_years\",\n",
    "    \"savings_assets\", \"current_debt\", \"loan_amount\", \"interest_rate\"\n",
    "]\n",
    "cat_features_base = [\"occupation_status\", \"product_type\", \"loan_intent\"]\n",
    "\n",
    "cols_baseline = numeric_features_base + cat_features_base + ['loan_status']\n",
    "df_baseline = df_features[cols_baseline].copy()\n",
    "\n",
    "# Supprimer NaN (pour comparaison équitable avec C1)\n",
    "df_baseline = df_baseline.dropna()\n",
    "\n",
    "# One-hot encoding\n",
    "df_baseline_encoded = pd.get_dummies(df_baseline, columns=cat_features_base, prefix_sep=\"__\", drop_first=False)\n",
    "\n",
    "X_baseline = df_baseline_encoded.drop(columns=['loan_status'])\n",
    "y_baseline = df_baseline_encoded['loan_status'].astype(int)\n",
    "\n",
    "# Standardisation\n",
    "scaler_baseline = StandardScaler()\n",
    "X_baseline_scaled = scaler_baseline.fit_transform(X_baseline)\n",
    "\n",
    "print(f\"Dataset: {X_baseline_scaled.shape}\")\n",
    "print(f\"Distribution: {y_baseline.value_counts(normalize=True).to_dict()}\")\n",
    "\n",
    "# Modèle avec class_weight='balanced' (SANS SMOTE)\n",
    "model_e1 = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Validation croisée Stratified K-Fold (k=5)\n",
    "print(\"\\n--- VALIDATION CROISEE (Stratified 5-Fold) ---\")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "cv_results_e1 = cross_validate(model_e1, X_baseline_scaled, y_baseline, \n",
    "                                cv=skf, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "print(\"\\nResultats Cross-Validation:\")\n",
    "print(f\"  Accuracy: {cv_results_e1['test_accuracy'].mean():.4f} (+/- {cv_results_e1['test_accuracy'].std():.4f})\")\n",
    "print(f\"  Precision:{cv_results_e1['test_precision'].mean():.4f} (+/- {cv_results_e1['test_precision'].std():.4f})\")\n",
    "print(f\"  Recall: {cv_results_e1['test_recall'].mean():.4f} (+/- {cv_results_e1['test_recall'].std():.4f})\")\n",
    "print(f\"  F1-Score:  {cv_results_e1['test_f1'].mean():.4f} (+/- {cv_results_e1['test_f1'].std():.4f})\")\n",
    "print(f\"  ROC-AUC:   {cv_results_e1['test_roc_auc'].mean():.4f} (+/- {cv_results_e1['test_roc_auc'].std():.4f})\")\n",
    "\n",
    "# Entraînement sur tout le dataset pour évaluation finale\n",
    "print(\"\\n--- ENTRAINEMENT ET EVALUATION FINALE ---\")\n",
    "X_train_e1, X_test_e1, y_train_e1, y_test_e1 = train_test_split(\n",
    "    X_baseline_scaled, y_baseline, test_size=0.2, random_state=42, stratify=y_baseline\n",
    ")\n",
    "\n",
    "model_e1.fit(X_train_e1, y_train_e1)\n",
    "y_pred_e1 = model_e1.predict(X_test_e1)\n",
    "y_pred_proba_e1 = model_e1.predict_proba(X_test_e1)[:, 1]\n",
    "\n",
    "# Métriques finales\n",
    "f1_e1 = f1_score(y_test_e1, y_pred_e1)\n",
    "precision_e1 = precision_score(y_test_e1, y_pred_e1)\n",
    "recall_e1 = recall_score(y_test_e1, y_pred_e1)\n",
    "accuracy_e1 = accuracy_score(y_test_e1, y_pred_e1)\n",
    "fpr_e1, tpr_e1, _ = roc_curve(y_test_e1, y_pred_proba_e1)\n",
    "auc_e1 = auc(fpr_e1, tpr_e1)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  F1-Score:  {f1_e1:.4f}\")\n",
    "print(f\"  Precision: {precision_e1:.4f}\")\n",
    "print(f\"  Recall:    {recall_e1:.4f}\")\n",
    "print(f\"  Accuracy:  {accuracy_e1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {auc_e1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"E1 TERMINE - Baseline SANS SMOTE\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ukbk11p2ivi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== E2: AMELIORE COMPLET (KNN + Features + SMOTE) ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"E2: AMELIORE COMPLET (KNN + FEATURES INTERACTION + SMOTE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Utiliser toutes les features (comme D1)\n",
    "all_numeric_features_e2 = numeric_features + new_ratios + interaction_features\n",
    "cols_e2 = all_numeric_features_e2 + cat_features + ['loan_status']\n",
    "df_e2 = df_features[cols_e2].copy()\n",
    "\n",
    "print(f\"Dataset initial: {df_e2.shape}\")\n",
    "\n",
    "# KNN Imputation pour features numériques\n",
    "print(\"\\n--- KNN IMPUTATION ---\")\n",
    "missing_before = df_e2[all_numeric_features_e2].isnull().sum().sum()\n",
    "print(f\"Valeurs manquantes avant: {missing_before}\")\n",
    "\n",
    "if missing_before > 0:\n",
    "    knn_imputer_e2 = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    df_e2[all_numeric_features_e2] = knn_imputer_e2.fit_transform(df_e2[all_numeric_features_e2])\n",
    "    print(\"KNN Imputer applique\")\n",
    "\n",
    "# One-hot encoding\n",
    "df_e2_encoded = pd.get_dummies(df_e2, columns=cat_features, prefix_sep=\"__\", drop_first=False)\n",
    "\n",
    "X_e2 = df_e2_encoded.drop(columns=['loan_status'])\n",
    "y_e2 = df_e2_encoded['loan_status'].astype(int)\n",
    "\n",
    "print(f\"\\nDataset final: {X_e2.shape}\")\n",
    "print(f\"Distribution: {y_e2.value_counts(normalize=True).to_dict()}\")\n",
    "\n",
    "# Standardisation\n",
    "scaler_e2 = StandardScaler()\n",
    "X_e2_scaled = scaler_e2.fit_transform(X_e2)\n",
    "\n",
    "# Validation croisée avec SMOTE dans le pipeline\n",
    "print(\"\\n--- VALIDATION CROISEE AVEC SMOTE (Stratified 5-Fold) ---\")\n",
    "print(\"Note: SMOTE applique UNIQUEMENT sur le train fold de chaque iteration\")\n",
    "\n",
    "skf_e2 = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Pipeline imblearn pour appliquer SMOTE UNIQUEMENT sur le train set\n",
    "# Attention: cross_validate ne fonctionne pas directement avec Pipeline imblearn\n",
    "# On doit faire une boucle manuelle\n",
    "\n",
    "cv_scores_e2 = {\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'roc_auc': []\n",
    "}\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf_e2.split(X_e2_scaled, y_e2), 1):\n",
    "    X_train_fold = X_e2_scaled[train_idx]\n",
    "    y_train_fold = y_e2.iloc[train_idx]\n",
    "    X_test_fold = X_e2_scaled[test_idx]\n",
    "    y_test_fold = y_e2.iloc[test_idx]\n",
    "    \n",
    "    # Apply SMOTE only on train fold\n",
    "    smote_fold = SMOTE(random_state=42, k_neighbors=5)\n",
    "    X_train_resampled, y_train_resampled = smote_fold.fit_resample(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Train model\n",
    "    model_fold = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs', class_weight='balanced')\n",
    "    model_fold.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict on test fold (NOT resampled)\n",
    "    y_pred_fold = model_fold.predict(X_test_fold)\n",
    "    y_pred_proba_fold = model_fold.predict_proba(X_test_fold)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cv_scores_e2['accuracy'].append(accuracy_score(y_test_fold, y_pred_fold))\n",
    "    cv_scores_e2['precision'].append(precision_score(y_test_fold, y_pred_fold))\n",
    "    cv_scores_e2['recall'].append(recall_score(y_test_fold, y_pred_fold))\n",
    "    cv_scores_e2['f1'].append(f1_score(y_test_fold, y_pred_fold))\n",
    "    cv_scores_e2['roc_auc'].append(roc_auc_score(y_test_fold, y_pred_proba_fold))\n",
    "    \n",
    "    print(f\"  Fold {fold}: F1={cv_scores_e2['f1'][-1]:.4f}, AUC={cv_scores_e2['roc_auc'][-1]:.4f}\")\n",
    "\n",
    "# Moyennes et écarts-types\n",
    "e2_cv_results = {\n",
    "    'accuracy': cv_scores_e2['accuracy'],\n",
    "    'precision': cv_scores_e2['precision'],\n",
    "    'recall': cv_scores_e2['recall'],\n",
    "    'f1': cv_scores_e2['f1'],\n",
    "    'roc_auc': cv_scores_e2['roc_auc']\n",
    "}\n",
    "\n",
    "print(\"\\n--- RESULTATS CROSS-VALIDATION (5-FOLD) ---\")\n",
    "print(f\"  Accuracy:  {np.mean(e2_cv_results['accuracy']):.4f} ± {np.std(e2_cv_results['accuracy']):.4f}\")\n",
    "print(f\"  Precision: {np.mean(e2_cv_results['precision']):.4f} ± {np.std(e2_cv_results['precision']):.4f}\")\n",
    "print(f\"  Recall:    {np.mean(e2_cv_results['recall']):.4f} ± {np.std(e2_cv_results['recall']):.4f}\")\n",
    "print(f\"  F1-Score:  {np.mean(e2_cv_results['f1']):.4f} ± {np.std(e2_cv_results['f1']):.4f}\")\n",
    "print(f\"  AUC-ROC:   {np.mean(e2_cv_results['roc_auc']):.4f} ± {np.std(e2_cv_results['roc_auc']):.4f}\")\n",
    "\n",
    "# Train final model on full training set for comparison\n",
    "X_train_e2, X_test_e2, y_train_e2, y_test_e2 = train_test_split(\n",
    "    X_e2_scaled, y_e2, test_size=0.2, random_state=42, stratify=y_e2\n",
    ")\n",
    "\n",
    "# Apply SMOTE on training set\n",
    "smote_final = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_e2_resampled, y_train_e2_resampled = smote_final.fit_resample(X_train_e2, y_train_e2)\n",
    "\n",
    "# Train final model\n",
    "log_reg_complete = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs', class_weight='balanced')\n",
    "log_reg_complete.fit(X_train_e2_resampled, y_train_e2_resampled)\n",
    "\n",
    "# Predictions\n",
    "y_pred_e2 = log_reg_complete.predict(X_test_e2)\n",
    "y_pred_proba_e2 = log_reg_complete.predict_proba(X_test_e2)[:, 1]\n",
    "\n",
    "# Final test set metrics\n",
    "f1_e2 = f1_score(y_test_e2, y_pred_e2)\n",
    "precision_e2 = precision_score(y_test_e2, y_pred_e2)\n",
    "recall_e2 = recall_score(y_test_e2, y_pred_e2)\n",
    "accuracy_e2 = accuracy_score(y_test_e2, y_pred_e2)\n",
    "auc_e2 = roc_auc_score(y_test_e2, y_pred_proba_e2)\n",
    "\n",
    "print(\"\\n--- RESULTATS SUR TEST SET FINAL ---\")\n",
    "print(f\"  Accuracy:  {accuracy_e2:.4f}\")\n",
    "print(f\"  Precision: {precision_e2:.4f}\")\n",
    "print(f\"  Recall:    {recall_e2:.4f}\")\n",
    "print(f\"  F1-Score:  {f1_e2:.4f}\")\n",
    "print(f\"  AUC-ROC:   {auc_e2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"E2 TERMINE\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1ru44flchf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ETAPE E3: COMPARAISON FINALE DES 4 APPROCHES ==========\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(\"E3: COMPARAISON FINALE DES 4 APPROCHES DE MODELISATION\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# ========== AGGREGATION DES RESULTATS ==========\n",
    "\n",
    "# Modele C: Baseline + SMOTE (valeurs de C3)\n",
    "# Variables: f1, precision, recall, accuracy, roc_auc\n",
    "c_f1 = f1\n",
    "c_precision = precision\n",
    "c_recall = recall\n",
    "c_accuracy = accuracy\n",
    "c_auc = roc_auc\n",
    "\n",
    "# Modele D: KNN+Features SANS SMOTE (valeurs de D3)\n",
    "# Variables: f1_v2, precision_v2, recall_v2, accuracy_v2, auc_v2\n",
    "d_f1 = f1_v2\n",
    "d_precision = precision_v2\n",
    "d_recall = recall_v2\n",
    "d_accuracy = accuracy_v2\n",
    "d_auc = auc_v2\n",
    "\n",
    "# Modele E1: Baseline SANS SMOTE (avec CV)\n",
    "e1_f1_mean = cv_results_e1['test_f1'].mean()\n",
    "e1_f1_std = cv_results_e1['test_f1'].std()\n",
    "e1_precision_mean = cv_results_e1['test_precision'].mean()\n",
    "e1_precision_std = cv_results_e1['test_precision'].std()\n",
    "e1_recall_mean = cv_results_e1['test_recall'].mean()\n",
    "e1_recall_std = cv_results_e1['test_recall'].std()\n",
    "e1_accuracy_mean = cv_results_e1['test_accuracy'].mean()\n",
    "e1_accuracy_std = cv_results_e1['test_accuracy'].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3891c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir feature_names depuis best_X_test (meilleur modèle)\n",
    "# IMPORTANT: Utiliser best_X_test pour garantir cohérence avec best_model d'une étape précédente\n",
    "try:\n",
    "    if hasattr(best_X_test, 'columns'):\n",
    "        feature_names = best_X_test.columns.tolist()\n",
    "    else:\n",
    "        # Si best_X_test est un numpy array\n",
    "        feature_names = [f'feature_{i}' for i in range(best_X_test.shape[1])]\n",
    "    print(f\" feature_names défini: {len(feature_names)} features\")\n",
    "except NameError:\n",
    "    # best_X_test pas encore défini, utiliser les features du dataset original\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('Loan_approval_data_2025.csv')\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    if 'loan_status' in numeric_cols:\n",
    "        numeric_cols.remove('loan_status')\n",
    "    feature_names = numeric_cols\n",
    "    print(f\" feature_names défini depuis dataset: {len(feature_names)} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SÉLECTION DU MEILLEUR MODÈLE ET DATASETS POUR SECTION F ==========\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SÉLECTION DU MEILLEUR MODÈLE ET DÉFINITION DES VARIABLES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# ========== ÉTAPE 1: SÉLECTION DU MODÈLE ==========\n",
    "# Ordre de préférence: E2 (complet) > E1 (baseline no SMOTE) > D (improved) > C (baseline)\n",
    "\n",
    "best_model = None\n",
    "best_model_name = None\n",
    "best_source = None\n",
    "\n",
    "# Vérifier E2 (Pipeline complet: KNN + Features + SMOTE)\n",
    "if 'log_reg_complete' in locals():\n",
    "    best_model = log_reg_complete\n",
    "    best_model_name = 'log_reg_complete'\n",
    "    best_source = 'E2'\n",
    "    print(\"Modèle E2 trouvé: log_reg_complete (KNN + Features + SMOTE)\")\n",
    "\n",
    "# Fallback sur E1 (Baseline sans SMOTE)\n",
    "elif 'log_reg_e1' in locals():\n",
    "    best_model = log_reg_e1\n",
    "    best_model_name = 'log_reg_e1'\n",
    "    best_source = 'E1'\n",
    "    print(\"  Modèle E1 trouvé: log_reg_e1 (Baseline sans SMOTE)\")\n",
    "\n",
    "# Fallback sur D (Improved avec KNN)\n",
    "elif 'log_reg_v2' in locals():\n",
    "    best_model = log_reg_v2\n",
    "    best_model_name = 'log_reg_v2'\n",
    "    best_source = 'D'\n",
    "    print(\" Modèle D trouvé: log_reg_v2 (KNN Imputer)\")\n",
    "\n",
    "# Fallback sur C (Baseline avec SMOTE)\n",
    "elif 'log_reg' in locals():\n",
    "    best_model = log_reg\n",
    "    best_model_name = 'log_reg'\n",
    "    best_source = 'C'\n",
    "    print(\" Modèle C trouvé: log_reg (Baseline + SMOTE)\")\n",
    "\n",
    "else:\n",
    "    print(\"ERREUR: Aucun modèle trouvé!\")\n",
    "    print(\"   Solution: Exécuter les cellules des Sections C, D, E1 ou E2 d'abord\")\n",
    "    best_model = None\n",
    "\n",
    "print()\n",
    "\n",
    "# ========== ÉTAPE 2: DÉFINITION DES DATASETS ==========\n",
    "if best_model is not None:\n",
    "    best_X_train = None\n",
    "    best_X_test = None\n",
    "    best_y_train = None\n",
    "    best_y_test = None\n",
    "    \n",
    "    # Mapper source → variables correspondantes\n",
    "    if best_source == 'E2' and 'X_train_e2' in locals():\n",
    "        best_X_train = X_train_e2\n",
    "        best_X_test = X_test_e2\n",
    "        best_y_train = y_train_e2\n",
    "        best_y_test = y_test_e2\n",
    "        print(f\" Variables E2 assignées:\")\n",
    "        print(f\"   - best_X_train: {best_X_train.shape}\")\n",
    "        print(f\"   - best_X_test: {best_X_test.shape}\")\n",
    "        print(f\"   - best_y_train: {len(best_y_train):,} samples\")\n",
    "        print(f\"   - best_y_test: {len(best_y_test):,} samples\")\n",
    "    \n",
    "    elif best_source == 'E1' and 'X_train_e1' in locals():\n",
    "        best_X_train = X_train_e1\n",
    "        best_X_test = X_test_e1\n",
    "        best_y_train = y_train_e1\n",
    "        best_y_test = y_test_e1\n",
    "        print(f\"Variables E1 assignées:\")\n",
    "        print(f\"   - best_X_train: {best_X_train.shape}\")\n",
    "        print(f\"   - best_X_test: {best_X_test.shape}\")\n",
    "    \n",
    "    elif best_source == 'D' and 'X_train_v2' in locals():\n",
    "        best_X_train = X_train_v2\n",
    "        best_X_test = X_test_v2\n",
    "        best_y_train = y_train_v2\n",
    "        best_y_test = y_test_v2\n",
    "        print(f\" Variables D assignées:\")\n",
    "        print(f\"   - best_X_train: {best_X_train.shape}\")\n",
    "        print(f\"   - best_X_test: {best_X_test.shape}\")\n",
    "    \n",
    "    elif best_source == 'C' and 'X_train' in locals():\n",
    "        best_X_train = X_train_scaled\n",
    "        best_X_test = X_test_scaled\n",
    "        best_y_train = y_train\n",
    "        best_y_test = y_test\n",
    "        print(f\" Variables C assignées:\")\n",
    "        print(f\"   - best_X_train: {best_X_train.shape}\")\n",
    "        print(f\"   - best_X_test: {best_X_test.shape}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\" ERREUR: Variables pour {best_source} non trouvées!\")\n",
    "        print(f\"   Expected: X_train_{best_source.lower()}, X_test_{best_source.lower()}\")\n",
    "        best_model = None\n",
    "    \n",
    "    print()\n",
    "\n",
    "# ========== ÉTAPE 3: RÉSUMÉ ==========\n",
    "print(\"=\"*80)\n",
    "print(\"RÉSUMÉ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_model is not None and best_X_train is not None:\n",
    "    print(f\" Configuration réussie:\")\n",
    "    print(f\"   - Source: Section {best_source}\")\n",
    "    print(f\"   - Modèle: {best_model_name}\")\n",
    "    print(f\"   - Type: {type(best_model).__name__}\")\n",
    "    print(f\"   - Features: {best_X_test.shape[1]}\")\n",
    "    print(f\"   - Test samples: {len(best_y_test):,}\")\n",
    "    print()\n",
    "    print(\"→ Section F peut maintenant utiliser:\")\n",
    "    print(\"   • best_model (modèle entraîné)\")\n",
    "    print(\"   • best_X_train, best_X_test (features)\")\n",
    "    print(\"   • best_y_train, best_y_test (labels)\")\n",
    "else:\n",
    "    print(\"❌ Configuration échouée\")\n",
    "    print(\"   Les cellules Section F seront ignorées\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e62769",
   "metadata": {},
   "source": [
    "#  SECTION F: ANALYSE AVANCÉE - MÉTRIQUES BANCAIRES ET INTERPRÉTABILITÉ\n",
    "\n",
    "Cette section implémente les **standards professionnels du secteur bancaire**:\n",
    "\n",
    "1. **Métriques Sectorielles**: Gini, KS Statistic, CAP Curve\n",
    "2. **Interprétabilité Réglementaire**: SHAP pour explicabilité RGPD\n",
    "3. **Analyse Profit/Risque**: Optimisation du seuil selon coûts business\n",
    "4. **Scorecard**: Système de points pour transparence\n",
    "5. **Monitoring**: PSI pour détection de drift\n",
    "6. **Tests de Discrimination**: Analyse de biais\n",
    "\n",
    "---\n",
    "\n",
    "##  Contexte\n",
    "\n",
    "Les métriques ML classiques (accuracy, F1) sont **insuffisantes** pour le credit scoring bancaire. \n",
    "\n",
    "Ce secteur exige:\n",
    "- **Gini/KS**: Métriques standard industrie\n",
    "- **Explicabilité**: RGPD Article 22 (droit à l'explication)\n",
    "- **Optimisation profit**: Pas seulement maximiser accuracy\n",
    "- **Transparence**: Scorecard lisible par auditeurs\n",
    "- **Stabilité**: Tests PSI pour production\n",
    "\n",
    "**Objectif:** Transformer notre modèle ML en système bancaire production-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca4be8",
   "metadata": {},
   "source": [
    "## F1. MÉTRIQUES SECTORIELLES BANCAIRES\n",
    "\n",
    "###  Pourquoi ces métriques ?\n",
    "\n",
    "Les banques et institutions financières utilisent des métriques spécifiques, différentes du ML traditionnel:\n",
    "\n",
    "| Métrique | Description | Interprétation | Seuil Industrie |\n",
    "|----------|-------------|----------------|------------------|\n",
    "| **Gini Coefficient** | 2×AUC - 1 | Pouvoir discriminant global | > 0.40 (bon) |\n",
    "| **KS Statistic** | Distance max entre cumulative distributions | Séparation classes | > 0.30 (acceptable) |\n",
    "| **CAP Curve** | Cumulative Accuracy Profile | Gain vs aléatoire | AR > 0.60 |\n",
    "\n",
    "**Avantages:**\n",
    "- Standardisées dans l'industrie (comparabilité)\n",
    "- Indépendantes du seuil de décision\n",
    "- Interprétables par les régulateurs\n",
    "\n",
    "###  Rappel: Modèle à Évaluer\n",
    "\n",
    "Nous allons évaluer le **meilleur modèle de la Section E**:\n",
    "- Modèle: Régression Logistique\n",
    "- Features: KNN Imputer + Ratios + Interactions\n",
    "- Sampling: SMOTE\n",
    "- Validation: StratifiedKFold (5 folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424666ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Récupérer le meilleur modèle de la section E\n",
    "# Pour cet exemple, on suppose que les variables suivantes existent:\n",
    "# - best_model: le modèle entraîné\n",
    "# - best_X_test: features de test\n",
    "# - best_y_test: labels de test\n",
    "\n",
    "# Si ces variables n'existent pas, il faudra ré-entraîner le modèle\n",
    "# Vérification\n",
    "try:\n",
    "    print(\"Variables disponibles:\")\n",
    "    print(f\"  - best_model: {type(best_model)}\")\n",
    "    print(f\"  - X_test shape: {best_X_test.shape}\")\n",
    "    print(f\"  - best_y_test shape: {best_y_test.shape}\")\n",
    "    model_available = True\n",
    "except NameError:\n",
    "    print(\" Variables du modèle non disponibles\")\n",
    "    print(\"   Solution: Ré-exécuter les cellules de la Section E\")\n",
    "    model_available = False\n",
    "\n",
    "# Générer les prédictions probabilistes\n",
    "if model_available:\n",
    "    y_pred_proba = best_model.predict_proba(best_X_test)[:, 1]\n",
    "    y_pred = best_model.predict(best_X_test)\n",
    "    \n",
    "    # Calcul AUC-ROC\n",
    "    auc_roc = roc_auc_score(best_y_test, y_pred_proba)\n",
    "    \n",
    "    # Calcul Gini Coefficient\n",
    "    gini = 2 * auc_roc - 1\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"MÉTRIQUE 1: GINI COEFFICIENT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "    print(f\"Gini Coefficient: {gini:.4f}\")\n",
    "    print()\n",
    "    print(\"Interprétation:\")\n",
    "    if gini >= 0.60:\n",
    "        print(\"   EXCELLENT (Gini ≥ 0.60)\")\n",
    "    elif gini >= 0.40:\n",
    "        print(\"   BON (Gini ≥ 0.40)\")\n",
    "    elif gini >= 0.30:\n",
    "        print(\"   ACCEPTABLE (Gini ≥ 0.30)\")\n",
    "    else:\n",
    "        print(\"   INSUFFISANT (Gini < 0.30)\")\n",
    "    print()\n",
    "    print(\"Contexte Industrie:\")\n",
    "    print(\"  - Gini < 0.30: Modèle peu discriminant\")\n",
    "    print(\"  - Gini 0.30-0.40: Acceptable pour screening initial\")\n",
    "    print(\"  - Gini 0.40-0.60: Bon pouvoir prédictif (standard)\")\n",
    "    print(\"  - Gini > 0.60: Excellent (rare, vérifier overfitting)\")\n",
    "else:\n",
    "    print(\"Impossible de calculer Gini sans modèle entraîné\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc20a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul KS Statistic (Kolmogorov-Smirnov)\n",
    "if model_available:\n",
    "    # Séparer les probabilités par classe\n",
    "    proba_class_0 = y_pred_proba[best_y_test == 0]  # Non approuvés\n",
    "    proba_class_1 = y_pred_proba[best_y_test == 1]  # Approuvés\n",
    "    \n",
    "    # Test KS\n",
    "    ks_stat, ks_pvalue = ks_2samp(proba_class_0, proba_class_1)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"MÉTRIQUE 2: KS STATISTIC (KOLMOGOROV-SMIRNOV)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"KS Statistic: {ks_stat:.4f}\")\n",
    "    print(f\"p-value: {ks_pvalue:.4e}\")\n",
    "    print()\n",
    "    print(\"Interprétation:\")\n",
    "    if ks_stat >= 0.40:\n",
    "        print(\"   EXCELLENT (KS ≥ 0.40)\")\n",
    "    elif ks_stat >= 0.30:\n",
    "        print(\"   BON (KS ≥ 0.30)\")\n",
    "    elif ks_stat >= 0.20:\n",
    "        print(\"   ACCEPTABLE (KS ≥ 0.20)\")\n",
    "    else:\n",
    "        print(\"   INSUFFISANT (KS < 0.20)\")\n",
    "    print()\n",
    "    print(\"Signification statistique:\")\n",
    "    if ks_pvalue < 0.001:\n",
    "        print(\"   Différence TRÈS significative (p < 0.001)\")\n",
    "    elif ks_pvalue < 0.05:\n",
    "        print(\"   Différence significative (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"   Différence non significative (p ≥ 0.05)\")\n",
    "    print()\n",
    "    print(\"Contexte:\")\n",
    "    print(\"  KS mesure la distance max entre les distributions cumulatives\")\n",
    "    print(\"  des deux classes. Plus KS est élevé, mieux le modèle sépare.\")\n",
    "    print()\n",
    "    print(f\"  Tailles des groupes:\")\n",
    "    print(f\"    - Classe 0 (refusés): {len(proba_class_0)} observations\")\n",
    "    print(f\"    - Classe 1 (approuvés): {len(proba_class_1)} observations\")\n",
    "    \n",
    "    # Visualisation des distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogrammes\n",
    "    axes[0].hist(proba_class_0, bins=30, alpha=0.6, label='Refusés (0)', color='red', density=True)\n",
    "    axes[0].hist(proba_class_1, bins=30, alpha=0.6, label='Approuvés (1)', color='green', density=True)\n",
    "    axes[0].set_xlabel('Probabilité Prédite', fontsize=12)\n",
    "    axes[0].set_ylabel('Densité', fontsize=12)\n",
    "    axes[0].set_title('Distribution des Probabilités par Classe', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distributions cumulatives\n",
    "    sorted_proba_0 = np.sort(proba_class_0)\n",
    "    sorted_proba_1 = np.sort(proba_class_1)\n",
    "    cumul_0 = np.arange(1, len(sorted_proba_0)+1) / len(sorted_proba_0)\n",
    "    cumul_1 = np.arange(1, len(sorted_proba_1)+1) / len(sorted_proba_1)\n",
    "    \n",
    "    axes[1].plot(sorted_proba_0, cumul_0, label='Refusés (0)', color='red', linewidth=2)\n",
    "    axes[1].plot(sorted_proba_1, cumul_1, label='Approuvés (1)', color='green', linewidth=2)\n",
    "    axes[1].set_xlabel('Probabilité Prédite', fontsize=12)\n",
    "    axes[1].set_ylabel('Proportion Cumulative', fontsize=12)\n",
    "    axes[1].set_title(f'Distributions Cumulatives (KS={ks_stat:.3f})', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print()\n",
    "    print(\" Analyse Visuelle:\")\n",
    "    print(\"   Plus les courbes sont séparées, meilleur est le modèle.\")\n",
    "    print(\"   Le KS mesure la distance verticale maximale entre les courbes.\")\n",
    "else:\n",
    "    print(\"Impossible de calculer KS sans modèle entraîné\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c389bb",
   "metadata": {},
   "source": [
    "###  CAP Curve (Cumulative Accuracy Profile)\n",
    "\n",
    "La **CAP Curve** compare le modèle à:\n",
    "- Un classement **aléatoire** (ligne diagonale)\n",
    "- Un classement **parfait** (courbe qui monte instantanément)\n",
    "\n",
    "**Métrique dérivée: Accuracy Ratio (AR)**\n",
    "\n",
    "```\n",
    "AR = Aire sous CAP du modèle / Aire sous CAP parfaite\n",
    "```\n",
    "\n",
    "**Interprétation AR:**\n",
    "- AR > 0.70: Excellent\n",
    "- AR 0.60-0.70: Bon\n",
    "- AR 0.50-0.60: Acceptable\n",
    "- AR < 0.50: Insuffisant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul CAP Curve et Accuracy Ratio\n",
    "if model_available:\n",
    "    # Trier par probabilité décroissante\n",
    "    sorted_indices = np.argsort(y_pred_proba)[::-1]\n",
    "    best_y_test_sorted = best_y_test.iloc[sorted_indices].values\n",
    "    \n",
    "    # Calculer cumulative des positifs\n",
    "    n_total = len(best_y_test)\n",
    "    n_positives = best_y_test.sum()\n",
    "    \n",
    "    cumulative_positives = np.cumsum(best_y_test_sorted)\n",
    "    cumulative_proportion = cumulative_positives / n_positives\n",
    "    x_axis = np.arange(1, n_total + 1) / n_total\n",
    "    \n",
    "    # Modèle parfait\n",
    "    perfect_x = [0, n_positives/n_total, 1]\n",
    "    perfect_y = [0, 1, 1]\n",
    "    \n",
    "    # Modèle aléatoire (diagonale)\n",
    "    random_x = [0, 1]\n",
    "    random_y = [0, 1]\n",
    "    \n",
    "    # Calculer Accuracy Ratio (AR)\n",
    "    area_model = np.trapz(cumulative_proportion, x_axis)\n",
    "    area_random = 0.5  # Aire sous diagonale\n",
    "    area_perfect = 0.5 + (n_positives/n_total) * (1 - n_positives/n_total) / 2\n",
    "    \n",
    "    accuracy_ratio = (area_model - area_random) / (area_perfect - area_random)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"MÉTRIQUE 3: CAP CURVE & ACCURACY RATIO (AR)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Accuracy Ratio (AR): {accuracy_ratio:.4f}\")\n",
    "    print()\n",
    "    print(\"Interprétation:\")\n",
    "    if accuracy_ratio >= 0.70:\n",
    "        print(\"   EXCELLENT (AR ≥ 0.70)\")\n",
    "    elif accuracy_ratio >= 0.60:\n",
    "        print(\"   BON (AR ≥ 0.60)\")\n",
    "    elif accuracy_ratio >= 0.50:\n",
    "        print(\"   ACCEPTABLE (AR ≥ 0.50)\")\n",
    "    else:\n",
    "        print(\"   INSUFFISANT (AR < 0.50)\")\n",
    "    print()\n",
    "    print(\"Calculs:\")\n",
    "    print(f\"  - Aire sous courbe modèle: {area_model:.4f}\")\n",
    "    print(f\"  - Aire sous courbe parfaite: {area_perfect:.4f}\")\n",
    "    print(f\"  - Aire sous courbe aléatoire: {area_random:.4f}\")\n",
    "    print(f\"  - AR = (modèle - aléatoire) / (parfait - aléatoire)\")\n",
    "    \n",
    "    # Visualisation CAP Curve\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    plt.plot(x_axis, cumulative_proportion, label=f'Modèle (AR={accuracy_ratio:.3f})', \n",
    "             color='blue', linewidth=2.5)\n",
    "    plt.plot(perfect_x, perfect_y, label='Modèle Parfait', \n",
    "             color='green', linewidth=2, linestyle='--')\n",
    "    plt.plot(random_x, random_y, label='Modèle Aléatoire', \n",
    "             color='red', linewidth=2, linestyle=':')\n",
    "    \n",
    "    # Remplir les aires\n",
    "    plt.fill_between(x_axis, 0, cumulative_proportion, alpha=0.2, color='blue')\n",
    "    \n",
    "    plt.xlabel('Proportion de la Population (ordonnée par score)', fontsize=12)\n",
    "    plt.ylabel('Proportion de Positifs Capturés', fontsize=12)\n",
    "    plt.title('CAP Curve (Cumulative Accuracy Profile)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Annotation\n",
    "    plt.text(0.5, 0.3, f'Accuracy Ratio = {accuracy_ratio:.3f}', \n",
    "             fontsize=14, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print()\n",
    "    print(\" Interprétation Visuelle:\")\n",
    "    print(\"   Plus la courbe bleue est proche de la verte (parfaite), meilleur est le modèle.\")\n",
    "    print(\"   Si elle est proche de la rouge (aléatoire), le modèle n'apporte rien.\")\n",
    "else:\n",
    "    print(\"Impossible de calculer CAP sans modèle entraîné\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e99588",
   "metadata": {},
   "source": [
    "###  CONCLUSION - Métriques Sectorielles\n",
    "\n",
    "**Synthèse des 3 métriques bancaires:**\n",
    "\n",
    "| Métrique | Valeur | Seuil Industrie | Statut |\n",
    "|----------|--------|-----------------|--------|\n",
    "| Gini Coefficient | [À calculer] | ≥ 0.40 | [À évaluer] |\n",
    "| KS Statistic | [À calculer] | ≥ 0.30 | [À évaluer] |\n",
    "| Accuracy Ratio (AR) | [À calculer] | ≥ 0.60 | [À évaluer] |\n",
    "\n",
    "**Analyse Comparative:**\n",
    "\n",
    "Ces 3 métriques mesurent le **pouvoir discriminant** du modèle mais sous des angles différents:\n",
    "\n",
    "- **Gini**: Basé sur l'AUC-ROC, mesure la capacité à classer correctement\n",
    "- **KS**: Mesure la séparation maximale entre les distributions\n",
    "- **AR**: Mesure le gain par rapport à un classement aléatoire\n",
    "\n",
    "** Si les 3 métriques sont bonnes:** Confiance élevée dans le modèle\n",
    "\n",
    "** Si divergence:** Investiguer plus en profondeur\n",
    "\n",
    "---\n",
    "\n",
    "** Note Méthodologique:**\n",
    "\n",
    "Ces métriques sont **indépendantes du seuil de décision** (contrairement à Precision/Recall). \n",
    "Elles évaluent le **classement** produit par le modèle, pas les prédictions binaires.\n",
    "\n",
    " **Prochaine étape:** Optimiser le seuil de décision selon les coûts business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c0231",
   "metadata": {},
   "source": [
    "## F2. ANALYSE PROFIT/RISQUE ET OPTIMISATION DU SEUIL\n",
    "\n",
    "###  Pourquoi optimiser selon le profit ?\n",
    "\n",
    "**Problème:** Le seuil par défaut (0.5) maximise souvent l'accuracy, **PAS le profit**.\n",
    "\n",
    "En credit scoring, les erreurs ont des **coûts asymétriques**:\n",
    "\n",
    "| Décision | Réalité | Résultat | Coût/Gain |\n",
    "|----------|---------|----------|------------|\n",
    "| Approuver | Bon client |  Vrai Positif (TP) | **+2 000 €** (intérêts) |\n",
    "| Refuser | Mauvais client |  Vrai Négatif (TN) | **0 €** (évite perte) |\n",
    "| Approuver | Mauvais client |  Faux Positif (FP) | **-15 000 €** (défaut) |\n",
    "| Refuser | Bon client |  Faux Négatif (FN) | **-800 €** (opportunité) |\n",
    "\n",
    "**Constat:** Un FP coûte **19x plus cher** qu'un FN !\n",
    "\n",
    " Le seuil optimal n'est **PAS 0.5**, mais dépend de ces coûts.\n",
    "\n",
    "###  Objectif\n",
    "\n",
    "Trouver le seuil qui **maximise le profit total** (ou minimise la perte totale).\n",
    "\n",
    "**Formule du profit:**\n",
    "\n",
    "```\n",
    "Profit Total = (TP × gain_tp) + (TN × gain_tn) - (FP × cost_fp) - (FN × cost_fn)\n",
    "```\n",
    "\n",
    "Nous allons tester **tous les seuils possibles** et choisir celui qui maximise le profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des coûts business (à ajuster selon le contexte réel)\n",
    "COST_FP = 15000  # Coût d'un prêt approuvé qui fait défaut (perte capitale)\n",
    "COST_FN = 800    # Coût d'un bon client refusé (manque à gagner intérêts)\n",
    "GAIN_TP = 2000   # Gain d'un bon prêt approuvé (intérêts perçus)\n",
    "GAIN_TN = 0      # Pas de gain direct à refuser un mauvais client\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSE PROFIT/RISQUE - OPTIMISATION DU SEUIL\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\" Coûts Business Définis:\")\n",
    "print(f\"  - Coût Faux Positif (FP): {COST_FP:,} €\")\n",
    "print(f\"  - Coût Faux Négatif (FN): {COST_FN:,} €\")\n",
    "print(f\"  - Gain Vrai Positif (TP): {GAIN_TP:,} €\")\n",
    "print(f\"  - Gain Vrai Négatif (TN): {GAIN_TN:,} €\")\n",
    "print()\n",
    "print(f\"   Ratio FP/FN: {COST_FP/COST_FN:.1f}x\")\n",
    "print(\"     (Un FP coûte autant que\", int(COST_FP/COST_FN), \"FN)\")\n",
    "print()\n",
    "\n",
    "if model_available:\n",
    "    # Fonction de calcul du profit\n",
    "    def calculate_profit(y_true, y_pred, cost_fp, cost_fn, gain_tp, gain_tn):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        profit = (tp * gain_tp) + (tn * gain_tn) - (fp * cost_fp) - (fn * cost_fn)\n",
    "        return profit, tp, tn, fp, fn\n",
    "    \n",
    "    # Tester différents seuils\n",
    "    thresholds = np.linspace(0.05, 0.95, 91)  # 91 seuils de 0.05 à 0.95\n",
    "    profits = []\n",
    "    tps, tns, fps, fns = [], [], [], []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred_temp = (y_pred_proba >= threshold).astype(int)\n",
    "        profit, tp, tn, fp, fn = calculate_profit(\n",
    "            best_y_test, y_pred_temp, COST_FP, COST_FN, GAIN_TP, GAIN_TN\n",
    "        )\n",
    "        profits.append(profit)\n",
    "        tps.append(tp)\n",
    "        tns.append(tn)\n",
    "        fps.append(fp)\n",
    "        fns.append(fn)\n",
    "    \n",
    "    # Trouver le seuil optimal\n",
    "    optimal_idx = np.argmax(profits)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    max_profit = profits[optimal_idx]\n",
    "    \n",
    "    # Calculer profit avec seuil par défaut (0.5)\n",
    "    y_pred_default = (y_pred_proba >= 0.5).astype(int)\n",
    "    profit_default, tp_def, tn_def, fp_def, fn_def = calculate_profit(\n",
    "        best_y_test, y_pred_default, COST_FP, COST_FN, GAIN_TP, GAIN_TN\n",
    "    )\n",
    "    \n",
    "    print(\" RÉSULTATS OPTIMISATION:\")\n",
    "    print()\n",
    "    print(f\"Seuil Optimal: {optimal_threshold:.3f}\")\n",
    "    print(f\"Profit Maximal: {max_profit:,} €\")\n",
    "    print()\n",
    "    print(\"Comparaison seuil 0.5 (défaut) vs optimal:\")\n",
    "    print()\n",
    "    print(f\"{'Métrique':<20} {'Seuil 0.5':>15} {'Seuil Optimal':>15} {'Différence':>15}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Seuil':<20} {0.5:>15.3f} {optimal_threshold:>15.3f} {optimal_threshold-0.5:>15.3f}\")\n",
    "    print(f\"{'Profit Total (€)':<20} {profit_default:>15,} {max_profit:>15,} {max_profit-profit_default:>15,}\")\n",
    "    print(f\"{'Vrais Positifs':<20} {tp_def:>15} {tps[optimal_idx]:>15} {tps[optimal_idx]-tp_def:>15}\")\n",
    "    print(f\"{'Vrais Négatifs':<20} {tn_def:>15} {tns[optimal_idx]:>15} {tns[optimal_idx]-tn_def:>15}\")\n",
    "    print(f\"{'Faux Positifs':<20} {fp_def:>15} {fps[optimal_idx]:>15} {fps[optimal_idx]-fp_def:>15}\")\n",
    "    print(f\"{'Faux Négatifs':<20} {fn_def:>15} {fns[optimal_idx]:>15} {fns[optimal_idx]-fn_def:>15}\")\n",
    "    print()\n",
    "    \n",
    "    # Gain relatif\n",
    "    gain_percent = ((max_profit - profit_default) / abs(profit_default)) * 100 if profit_default != 0 else 0\n",
    "    print(f\" Gain par rapport au seuil par défaut: {gain_percent:+.1f}%\")\n",
    "    \n",
    "    if max_profit > profit_default:\n",
    "        print(f\"    Optimisation BÉNÉFIQUE: +{max_profit - profit_default:,} €\")\n",
    "    else:\n",
    "        print(f\"    Seuil 0.5 déjà proche de l'optimal\")\n",
    "    \n",
    "else:\n",
    "    print(\"Impossible d'optimiser sans modèle entraîné\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la courbe profit/seuil\n",
    "if model_available:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Courbe Profit vs Seuil\n",
    "    axes[0, 0].plot(thresholds, profits, linewidth=2.5, color='darkgreen')\n",
    "    axes[0, 0].axvline(optimal_threshold, color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Optimal ({optimal_threshold:.3f})')\n",
    "    axes[0, 0].axvline(0.5, color='gray', linestyle=':', linewidth=2, label='Défaut (0.5)')\n",
    "    axes[0, 0].scatter([optimal_threshold], [max_profit], color='red', s=200, zorder=5, \n",
    "                       edgecolor='black', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Seuil de Décision', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Profit Total (€)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_title('Courbe Profit vs Seuil de Décision', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=11)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axhline(0, color='black', linewidth=1)\n",
    "    \n",
    "    # 2. Évolution FP et FN\n",
    "    axes[0, 1].plot(thresholds, fps, label='Faux Positifs (FP)', color='red', linewidth=2)\n",
    "    axes[0, 1].plot(thresholds, fns, label='Faux Négatifs (FN)', color='orange', linewidth=2)\n",
    "    axes[0, 1].axvline(optimal_threshold, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "    axes[0, 1].axvline(0.5, color='gray', linestyle=':', linewidth=2, alpha=0.5)\n",
    "    axes[0, 1].set_xlabel('Seuil de Décision', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Nombre d\\'Erreurs', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_title('Évolution des Erreurs selon le Seuil', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend(fontsize=11)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Évolution TP et TN\n",
    "    axes[1, 0].plot(thresholds, tps, label='Vrais Positifs (TP)', color='green', linewidth=2)\n",
    "    axes[1, 0].plot(thresholds, tns, label='Vrais Négatifs (TN)', color='blue', linewidth=2)\n",
    "    axes[1, 0].axvline(optimal_threshold, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "    axes[1, 0].axvline(0.5, color='gray', linestyle=':', linewidth=2, alpha=0.5)\n",
    "    axes[1, 0].set_xlabel('Seuil de Décision', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Nombre de Prédictions Correctes', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_title('Évolution des Prédictions Correctes', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend(fontsize=11)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Trade-off Precision-Recall\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(best_y_test, y_pred_proba)\n",
    "    \n",
    "    axes[1, 1].plot(recall, precision, linewidth=2.5, color='purple')\n",
    "    \n",
    "    # Marquer le point optimal\n",
    "    y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "    prec_optimal = precision_score(best_y_test, y_pred_optimal)\n",
    "    rec_optimal = recall_score(best_y_test, y_pred_optimal)\n",
    "    \n",
    "    axes[1, 1].scatter([rec_optimal], [prec_optimal], color='red', s=200, zorder=5, \n",
    "                       edgecolor='black', linewidth=2, label=f'Optimal (seuil={optimal_threshold:.3f})')\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_title('Courbe Precision-Recall', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].legend(fontsize=11)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_xlim(0, 1)\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print()\n",
    "    print(\" INTERPRÉTATION DES GRAPHIQUES:\")\n",
    "    print()\n",
    "    print(\"1. PROFIT vs SEUIL (haut gauche):\")\n",
    "    print(\"   - Forme de la courbe montre la sensibilité au seuil\")\n",
    "    print(\"   - Le point rouge indique le seuil qui maximise le profit\")\n",
    "    print()\n",
    "    print(\"2. ERREURS vs SEUIL (haut droit):\")\n",
    "    print(\"   - FP diminuent quand le seuil augmente (on est plus strict)\")\n",
    "    print(\"   - FN augmentent quand le seuil augmente (on refuse plus)\")\n",
    "    print(\"   - Le seuil optimal équilibre selon les coûts\")\n",
    "    print()\n",
    "    print(\"3. PRÉDICTIONS CORRECTES (bas gauche):\")\n",
    "    print(\"   - TP diminuent si seuil trop élevé\")\n",
    "    print(\"   - TN augmentent si seuil plus élevé\")\n",
    "    print()\n",
    "    print(\"4. PRECISION-RECALL (bas droit):\")\n",
    "    print(\"   - Trade-off classique ML\")\n",
    "    print(\"   - Le point rouge montre où on se situe avec le seuil optimal\")\n",
    "else:\n",
    "    print(\"Impossible de visualiser sans modèle entraîné\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f78ef",
   "metadata": {},
   "source": [
    "###  CONCLUSION - Analyse Profit/Risque\n",
    "\n",
    "**Enseignements clés:**\n",
    "\n",
    "1. **Le seuil par défaut (0.5) n'est PAS optimal**\n",
    "   - Il maximise souvent l'accuracy, pas le profit\n",
    "   - En credit scoring, les coûts asymétriques changent l'équilibre\n",
    "\n",
    "2. **L'optimisation dépend du contexte business**\n",
    "   - Si COST_FP >> COST_FN : seuil optimal > 0.5 (être plus strict)\n",
    "   - Si COST_FP ≈ COST_FN : seuil optimal ≈ 0.5\n",
    "   - Si COST_FN >> COST_FP : seuil optimal < 0.5 (être plus permissif)\n",
    "\n",
    "3. **Impact business quantifié**\n",
    "   - Différence de profit mesurée en € (pas seulement en %)\n",
    "   - Justification claire auprès des stakeholders\n",
    "\n",
    "4. **Sensibilité au seuil**\n",
    "   - Courbe \"plate\" autour de l'optimal : décision robuste\n",
    "   - Courbe \"pointue\" : nécessite recalibrage fréquent\n",
    "\n",
    "---\n",
    "\n",
    "** Recommandation pour Déploiement:**\n",
    "\n",
    "```python\n",
    "# Dans le système de production\n",
    "OPTIMAL_THRESHOLD = [valeur calculée]  # À mettre à jour mensuellement\n",
    "\n",
    "def predict_loan_decision(features):\n",
    "    proba = model.predict_proba(features)[:, 1]\n",
    "    decision = (proba >= OPTIMAL_THRESHOLD).astype(int)\n",
    "    return decision, proba\n",
    "```\n",
    "\n",
    " **Attention:** Le seuil optimal doit être **recalibré régulièrement**:\n",
    "- Changement des coûts (taux d'intérêt, politique de crédit)\n",
    "- Changement de la population (drift)\n",
    "- Changement des objectifs business (croissance vs rentabilité)\n",
    "\n",
    "---\n",
    "\n",
    "**Prochaine étape:** Implémenter SHAP pour l'interprétabilité réglementaire (RGPD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "od7w9swpoas",
   "metadata": {},
   "source": [
    "## F3. TEST DE CALIBRATION (HOSMER-LEMESHOW)\n",
    "\n",
    "###  Pourquoi tester la calibration ?\n",
    "\n",
    "La **calibration** mesure si les probabilités prédites correspondent aux fréquences observées.\n",
    "\n",
    "**Exemple:**\n",
    "- Si le modèle prédit 30% de risque pour 100 clients\n",
    "- On devrait observer ~30 défauts réels\n",
    "\n",
    "###  Test de Hosmer-Lemeshow\n",
    "\n",
    "**Hypothèse nulle H0:** Le modèle est bien calibré\n",
    "\n",
    "**Interprétation:**\n",
    "- **p-value > 0.05**:  Modèle bien calibré (on accepte H0)\n",
    "- **p-value < 0.05**:  Modèle mal calibré (on rejette H0)\n",
    "\n",
    "**Méthodologie:**\n",
    "1. Diviser les prédictions en 10 groupes (déciles)\n",
    "2. Comparer événements observés vs attendus\n",
    "3. Calculer statistique χ² et p-value\n",
    "\n",
    "###   Importance en Banking\n",
    "\n",
    "Un modèle **mal calibré** est **DANGEREUX**:\n",
    "- Sous-estime les risques → Pertes financières\n",
    "- Surest les risques → Perte de clients\n",
    "- Non-conforme pour régulateur (Basel II/III)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6o467c1ryg9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== F3: TEST HOSMER-LEMESHOW POUR CALIBRATION ==========\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"F3. TEST DE CALIBRATION (HOSMER-LEMESHOW)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def hosmer_lemeshow_test(y_true, y_pred_proba, n_bins=10):\n",
    "    \"\"\"\n",
    "    Test de Hosmer-Lemeshow pour calibration\n",
    "    \n",
    "    Returns:\n",
    "        chi2_stat: Statistique chi²\n",
    "        p_value: P-value du test\n",
    "        calibration_table: Table détaillée\n",
    "    \"\"\"\n",
    "    from scipy.stats import chi2\n",
    "    \n",
    "    # Créer bins basés sur probabilités prédites\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred_proba\n",
    "    })\n",
    "    \n",
    "    # Diviser en bins de probabilités\n",
    "    df['bin'] = pd.qcut(df['y_pred'], q=n_bins, duplicates='drop')\n",
    "    \n",
    "    # Calculer observé vs attendu par bin\n",
    "    grouped = df.groupby('bin').agg({\n",
    "        'y_true': ['sum', 'count'],\n",
    "        'y_pred': 'mean'\n",
    "    })\n",
    "    \n",
    "    grouped.columns = ['Observed_Events', 'Total', 'Expected_Prob']\n",
    "    grouped['Expected_Events'] = grouped['Expected_Prob'] * grouped['Total']\n",
    "    grouped['Observed_NonEvents'] = grouped['Total'] - grouped['Observed_Events']\n",
    "    grouped['Expected_NonEvents'] = grouped['Total'] - grouped['Expected_Events']\n",
    "    \n",
    "    # Statistique χ²\n",
    "    chi2_stat = np.sum(\n",
    "        ((grouped['Observed_Events'] - grouped['Expected_Events'])**2) / (grouped['Expected_Events'] + 1e-10) +\n",
    "        ((grouped['Observed_NonEvents'] - grouped['Expected_NonEvents'])**2) / (grouped['Expected_NonEvents'] + 1e-10)\n",
    "    )\n",
    "    \n",
    "    # Degrés de liberté\n",
    "    df_chi2 = len(grouped) - 2\n",
    "    \n",
    "    # P-value\n",
    "    p_value = 1 - chi2.cdf(chi2_stat, df_chi2)\n",
    "    \n",
    "    # Table de calibration\n",
    "    grouped['Observed_Rate'] = grouped['Observed_Events'] / grouped['Total']\n",
    "    calibration_table = grouped.reset_index()\n",
    "    \n",
    "    return chi2_stat, p_value, calibration_table\n",
    "\n",
    "# Vérifier disponibilité du modèle\n",
    "if 'best_model' in locals() and best_model is not None:\n",
    "    print(\"\\n MODÈLE À ÉVALUER:\")\n",
    "    print(f\"   Type: {type(best_model).__name__}\")\n",
    "    print(f\"   Features: {best_X_test.shape[1]}\")\n",
    "    \n",
    "    # Obtenir probabilités\n",
    "    try:\n",
    "        y_pred_proba = best_model.predict_proba(best_X_test)[:, 1]\n",
    "    except:\n",
    "        y_pred_proba = best_model.decision_function(best_X_test)\n",
    "        # Convertir en probabilités via sigmoid\n",
    "        y_pred_proba = 1 / (1 + np.exp(-y_pred_proba))\n",
    "    \n",
    "    print(f\"\\n EXÉCUTION DU TEST HOSMER-LEMESHOW...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Calculer test\n",
    "    chi2_stat, p_value, calibration_table = hosmer_lemeshow_test(\n",
    "        best_y_test, y_pred_proba, n_bins=10\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n RÉSULTATS:\")\n",
    "    print(f\"   χ² Statistic: {chi2_stat:.4f}\")\n",
    "    print(f\"   P-value: {p_value:.4f}\")\n",
    "    print(f\"   Degrés de liberté: {len(calibration_table) - 2}\")\n",
    "    \n",
    "    print(f\"\\n INTERPRÉTATION:\")\n",
    "    if p_value > 0.05:\n",
    "        print(f\"    MODÈLE BIEN CALIBRÉ (p-value = {p_value:.4f} > 0.05)\")\n",
    "        print(\"   → Les probabilités prédites sont fiables\")\n",
    "        print(\"   → Conforme pour usage en production bancaire\")\n",
    "    else:\n",
    "        print(f\"    MODÈLE MAL CALIBRÉ (p-value = {p_value:.4f} < 0.05)\")\n",
    "        print(\"   → Les probabilités prédites NE sont PAS fiables\")\n",
    "        print(\"   → Nécessite re-calibration (Platt Scaling ou Isotonic Regression)\")\n",
    "        print(\"   →  NON RECOMMANDÉ pour production sans correction\")\n",
    "    \n",
    "    # Afficher table de calibration\n",
    "    print(f\"\\n TABLE DE CALIBRATION (par décile):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Simplifier affichage\n",
    "    calib_display = calibration_table[[\n",
    "        'Total', 'Observed_Events', 'Expected_Events', \n",
    "        'Observed_Rate', 'Expected_Prob'\n",
    "    ]].copy()\n",
    "    \n",
    "    calib_display['Observed_Events'] = calib_display['Observed_Events'].astype(int)\n",
    "    calib_display['Expected_Events'] = calib_display['Expected_Events'].round(1)\n",
    "    calib_display['Observed_Rate'] = (calib_display['Observed_Rate'] * 100).round(2)\n",
    "    calib_display['Expected_Prob'] = (calib_display['Expected_Prob'] * 100).round(2)\n",
    "    \n",
    "    calib_display.columns = ['N', 'Obs_Events', 'Exp_Events', 'Obs_Rate(%)', 'Exp_Rate(%)']\n",
    "    \n",
    "    print(calib_display.to_string(index=False))\n",
    "    \n",
    "    # Visualisation: Calibration Curve\n",
    "    print(f\"\\n VISUALISATION: COURBE DE CALIBRATION\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    from sklearn.calibration import calibration_curve\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Calibration Curve\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        best_y_test, y_pred_proba, n_bins=10, strategy='uniform'\n",
    "    )\n",
    "    \n",
    "    ax1.plot(mean_predicted_value, fraction_of_positives, 's-',\n",
    "            label='Model', color='steelblue', linewidth=2.5, markersize=10)\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', linewidth=2)\n",
    "    \n",
    "    ax1.set_xlabel('Mean Predicted Probability', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Fraction of Positives (True Rate)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Calibration Curve\\n(Closer to diagonal = Better)',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='best', fontsize=11)\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Ajouter note sur le graphique\n",
    "    if p_value > 0.05:\n",
    "        ax1.text(0.05, 0.95, f' Well Calibrated\\n(p={p_value:.4f})',\n",
    "                transform=ax1.transAxes, fontsize=11,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "    else:\n",
    "        ax1.text(0.05, 0.95, f' Poorly Calibrated\\n(p={p_value:.4f})',\n",
    "                transform=ax1.transAxes, fontsize=11,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "    \n",
    "    # Plot 2: Distribution des probabilités prédites\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    ax2.hist(y_pred_proba[best_y_test == 0], bins=50, alpha=0.6,\n",
    "            label='Good (Class 0)', color='green', edgecolor='black')\n",
    "    ax2.hist(y_pred_proba[best_y_test == 1], bins=50, alpha=0.6,\n",
    "            label='Bad (Class 1)', color='red', edgecolor='black')\n",
    "    \n",
    "    ax2.set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Distribution of Predicted Probabilities\\nby True Class',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='best', fontsize=11)\n",
    "    ax2.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"f3_hosmer_lemeshow_calibration.png\"),\n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n GRAPHIQUE SAUVEGARDÉ: f3_hosmer_lemeshow_calibration.png\")\n",
    "    \n",
    "    # Recommandations\n",
    "    print(f\"\\n RECOMMANDATIONS:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"    ACTIONS NÉCESSAIRES:\")\n",
    "        print(\"      1. Appliquer Platt Scaling (CalibratedClassifierCV)\")\n",
    "        print(\"      2. Ou utiliser Isotonic Regression pour re-calibration\")\n",
    "        print(\"      3. Vérifier distribution des features (outliers?)\")\n",
    "        print(\"      4. Considérer feature engineering supplémentaire\")\n",
    "        print(\"\\n    AVANT PRODUCTION:\")\n",
    "        print(\"      - Re-calibrer le modèle OBLIGATOIRE\")\n",
    "        print(\"      - Re-tester Hosmer-Lemeshow après calibration\")\n",
    "        print(\"      - Documenter la procédure de calibration\")\n",
    "    else:\n",
    "        print(\"    MODÈLE PRÊT POUR PRODUCTION\")\n",
    "        print(\"      - Calibration validée\")\n",
    "        print(\"      - Probabilités fiables pour scoring\")\n",
    "        print(\"      - Conforme standards bancaires\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" TEST HOSMER-LEMESHOW TERMINÉ\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "else:\n",
    "    print(\"\\n ERREUR: Modèle non disponible\")\n",
    "    print(\"   Solution: Ré-exécuter les cellules de la Section E\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9c5d4a",
   "metadata": {},
   "source": [
    "## F4. SCORECARD TRADITIONNEL (Système de Points)\n",
    "\n",
    "###  Qu'est-ce qu'un Scorecard ?\n",
    "\n",
    "Un **scorecard** est la représentation traditionnelle des modèles de credit scoring.\n",
    "\n",
    "**Principe:**\n",
    "- Chaque feature contribue un certain nombre de **points**\n",
    "- Le **score total** détermine la décision\n",
    "- Score élevé = Bon client, Score faible = Client risqué\n",
    "\n",
    "**Avantages:**\n",
    "-  Extrêmement transparent\n",
    "-  Calculable à la main\n",
    "-  Accepté par les régulateurs\n",
    "-  Communicable aux clients\n",
    "\n",
    "###  Construction du Scorecard\n",
    "\n",
    "**Méthode standard:**\n",
    "\n",
    "1. **Définir un score de base** (ex: 600 points)\n",
    "2. **Définir PDO** (Points to Double the Odds, ex: 50)\n",
    "3. **Convertir les coefficients** du modèle en points\n",
    "\n",
    "**Formule de conversion:**\n",
    "\n",
    "```\n",
    "Points(feature) = (coefficient × factor) + offset\n",
    "```\n",
    "\n",
    "Où:\n",
    "- `factor = PDO / ln(2)`\n",
    "- `offset = base_score - (intercept × factor)`\n",
    "\n",
    "###  Exemple d'utilisation\n",
    "\n",
    "```\n",
    "Client Jean Dupont:\n",
    "  - Credit Score: 750      → +120 points\n",
    "  - Revenu Annuel: 45000   → +80 points\n",
    "  - Dette/Revenu: 0.25     → +50 points\n",
    "  - ...\n",
    "  SCORE TOTAL: 680 points  → APPROUVÉ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46605808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du Scorecard\n",
    "if model_available:\n",
    "    # IMPORTANT: Récupérer les noms de features depuis best_X_test\n",
    "    # pour garantir cohérence avec best_model.coef_\n",
    "    if hasattr(best_X_test, 'columns'):\n",
    "        feature_names_scorecard = best_X_test.columns.tolist()\n",
    "    elif isinstance(best_X_test, pd.DataFrame):\n",
    "        feature_names_scorecard = best_X_test.columns.tolist()\n",
    "    else:\n",
    "        # Si array numpy, créer noms génériques\n",
    "        n_features = best_X_test.shape[1]\n",
    "        feature_names_scorecard = [f'feature_{i}' for i in range(n_features)]\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CONSTRUCTION DU SCORECARD\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    print(f\" Nombre de features: {len(feature_names_scorecard)}\")\n",
    "    print()\n",
    "    # Paramètres du scorecard\n",
    "    BASE_SCORE = 600\n",
    "    PDO = 50  # Points to Double the Odds\n",
    "    \n",
    "    print(f\"Paramètres:\")\n",
    "    print(f\"  - Score de base: {BASE_SCORE} points\")\n",
    "    print(f\"  - PDO (Points to Double the Odds): {PDO}\")\n",
    "    print()\n",
    "    \n",
    "    # Récupérer coefficients et intercept\n",
    "    coefficients = best_model.coef_[0]\n",
    "    intercept = best_model.intercept_[0]\n",
    "    \n",
    "    # Calculer factor et offset\n",
    "    factor = PDO / np.log(2)\n",
    "    offset_score = BASE_SCORE - (intercept * factor / len(coefficients))\n",
    "    \n",
    "    # Convertir coefficients en points\n",
    "    points = (coefficients * factor / len(coefficients)).astype(int)\n",
    "    \n",
    "    # Créer DataFrame scorecard\n",
    "    scorecard_df = pd.DataFrame({\n",
    "        'Feature': feature_names_scorecard,\n",
    "        'Coefficient': coefficients,\n",
    "        'Points': points,\n",
    "        'Abs_Points': np.abs(points)\n",
    "    }).sort_values('Abs_Points', ascending=False)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SCORECARD - TOP 20 FEATURES PAR IMPACT\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    print(f\"{'Feature':<40} {'Coefficient':>12} {'Points':>12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for idx, row in scorecard_df.head(20).iterrows():\n",
    "        print(f\"{row['Feature']:<40} {row['Coefficient']:>12.4f} {row['Points']:>12}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Score de base (offset): {int(offset_score)} points\")\n",
    "    print()\n",
    "    print(\" Interprétation:\")\n",
    "    print(\"  - Points POSITIFS: Augmentent le score (favorable à l'approbation)\")\n",
    "    print(\"  - Points NÉGATIFS: Diminuent le score (défavorable)\")\n",
    "    print()\n",
    "    \n",
    "    # Calculer scores pour l'échantillon de test\n",
    "    scores_test = []\n",
    "    for i in range(len(best_X_test)):\n",
    "        score = offset_score\n",
    "        for j, coef in enumerate(coefficients):\n",
    "            score += best_X_test[i, j] * (coef * factor / len(coefficients))\n",
    "        scores_test.append(score)\n",
    "    \n",
    "    scores_test = np.array(scores_test)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"DISTRIBUTION DES SCORES\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Score minimum: {scores_test.min():.0f}\")\n",
    "    print(f\"Score maximum: {scores_test.max():.0f}\")\n",
    "    print(f\"Score moyen: {scores_test.mean():.0f}\")\n",
    "    print(f\"Score médian: {np.median(scores_test):.0f}\")\n",
    "    print()\n",
    "    \n",
    "    # Visualisation distribution des scores\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogramme par classe\n",
    "    scores_class_0 = scores_test[best_y_test == 0]\n",
    "    scores_class_1 = scores_test[best_y_test == 1]\n",
    "    \n",
    "    axes[0].hist(scores_class_0, bins=30, alpha=0.6, label='Refusés (0)', color='red', density=True)\n",
    "    axes[0].hist(scores_class_1, bins=30, alpha=0.6, label='Approuvés (1)', color='green', density=True)\n",
    "    axes[0].set_xlabel('Score', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Densité', fontsize=12)\n",
    "    axes[0].set_title('Distribution des Scores par Classe', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Boxplot\n",
    "    data_boxplot = [scores_class_0, scores_class_1]\n",
    "    axes[1].boxplot(data_boxplot, labels=['Refusés', 'Approuvés'], patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightblue'),\n",
    "                    medianprops=dict(color='red', linewidth=2))\n",
    "    axes[1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Distribution des Scores - Boxplot', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print()\n",
    "    print(\" Analyse:\")\n",
    "    print(f\"  - Les clients APPROUVÉS ont un score moyen de {scores_class_1.mean():.0f}\")\n",
    "    print(f\"  - Les clients REFUSÉS ont un score moyen de {scores_class_0.mean():.0f}\")\n",
    "    print(f\"  - Écart: {scores_class_1.mean() - scores_class_0.mean():.0f} points\")\n",
    "    \n",
    "else:\n",
    "    print(\"Impossible de créer scorecard sans modèle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739ba59",
   "metadata": {},
   "source": [
    "## F5. MONITORING - PSI (Population Stability Index)\n",
    "\n",
    "###  Pourquoi le PSI est critique en production ?\n",
    "\n",
    "Le **Population Stability Index** détecte les **drifts** de données:\n",
    "- La population de clients change-t-elle ?\n",
    "- Les caractéristiques évoluent-elles ?\n",
    "- Le modèle reste-t-il valide ?\n",
    "\n",
    "**Exemple de drift:**\n",
    "- Après COVID-19, les revenus ont chuté\n",
    "- Les taux d'intérêt ont changé\n",
    "- La population demandant des prêts est différente\n",
    "\n",
    " Le modèle entraîné sur anciennes données peut devenir **obsolète**.\n",
    "\n",
    "###  Formule du PSI\n",
    "\n",
    "Pour chaque feature:\n",
    "\n",
    "```\n",
    "PSI = Σ (Actual% - Expected%) × ln(Actual% / Expected%)\n",
    "```\n",
    "\n",
    "**Interprétation:**\n",
    "- PSI < 0.10: **Pas de changement significatif** \n",
    "- PSI 0.10-0.25: **Drift modéré**  (à surveiller)\n",
    "- PSI > 0.25: **Drift significatif**  (re-entraînement nécessaire)\n",
    "\n",
    "###  Utilisation en Production\n",
    "\n",
    "**Monitoring mensuel recommandé:**\n",
    "1. Comparer distribution actuelle vs distribution d'entraînement\n",
    "2. Calculer PSI pour chaque feature\n",
    "3. Alerter si PSI > 0.25\n",
    "4. Déclencher re-entraînement si nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du PSI (Population Stability Index)\n",
    "if model_available:\n",
    "    def calculate_psi(expected, actual, bins=10, epsilon=1e-4):\n",
    "        \"\"\"\n",
    "        Calcule le PSI entre deux distributions\n",
    "        \n",
    "        Args:\n",
    "            expected: Distribution de référence (training)\n",
    "            actual: Distribution actuelle (production/test)\n",
    "            bins: Nombre de bins pour discrétiser\n",
    "            epsilon: Valeur minimale pour éviter division par 0\n",
    "        \n",
    "        Returns:\n",
    "            psi_value: Valeur PSI\n",
    "        \"\"\"\n",
    "        # Définir les bins basés sur expected\n",
    "        breakpoints = np.linspace(expected.min(), expected.max(), bins + 1)\n",
    "        breakpoints[0] = -np.inf\n",
    "        breakpoints[-1] = np.inf\n",
    "        \n",
    "        # Calculer les pourcentages pour chaque bin\n",
    "        expected_percents = np.histogram(expected, bins=breakpoints)[0] / len(expected)\n",
    "        actual_percents = np.histogram(actual, bins=breakpoints)[0] / len(actual)\n",
    "        \n",
    "        # Éviter les divisions par 0\n",
    "        expected_percents = np.where(expected_percents == 0, epsilon, expected_percents)\n",
    "        actual_percents = np.where(actual_percents == 0, epsilon, actual_percents)\n",
    "        \n",
    "        # Calculer PSI\n",
    "        psi_values = (actual_percents - expected_percents) * np.log(actual_percents / expected_percents)\n",
    "        psi = np.sum(psi_values)\n",
    "        \n",
    "        return psi\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CALCUL DU PSI (Population Stability Index)\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    print(\" SIMULATION: Nous comparons train vs test\")\n",
    "    print(\"   En production: comparer données initiales vs données actuelles\")\n",
    "    print()\n",
    "    \n",
    "    # Calculer PSI pour chaque feature\n",
    "    # NOTE: On simule en comparant X_train vs X_test\n",
    "    # En production, comparer X_train vs X_production\n",
    "    \n",
    "    try:\n",
    "        X_train_df = best_X_train if isinstance(best_X_train, pd.DataFrame) else pd.DataFrame(best_X_train, columns=feature_names)\n",
    "        X_test_df = best_X_test if isinstance(best_X_test, pd.DataFrame) else pd.DataFrame(best_X_test, columns=feature_names)\n",
    "        \n",
    "        psi_results = []\n",
    "        for feature in feature_names:\n",
    "            psi_value = calculate_psi(X_train_df[feature].values, X_test_df[feature].values)\n",
    "            psi_results.append({\n",
    "                'Feature': feature,\n",
    "                'PSI': psi_value,\n",
    "                'Status': 'OK' if psi_value < 0.10 else ('WARNING' if psi_value < 0.25 else 'CRITICAL')\n",
    "            })\n",
    "        \n",
    "        psi_df = pd.DataFrame(psi_results).sort_values('PSI', ascending=False)\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"RÉSULTATS PSI PAR FEATURE (Top 20)\")\n",
    "        print(\"=\"*80)\n",
    "        print()\n",
    "        print(f\"{'Feature':<40} {'PSI':>10} {'Status':>12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for idx, row in psi_df.head(20).iterrows():\n",
    "            status_symbol = '' if row['Status'] == 'OK' else ('' if row['Status'] == 'WARNING' else '')\n",
    "            print(f\"{row['Feature']:<40} {row['PSI']:>10.4f} {status_symbol} {row['Status']:>10}\")\n",
    "        \n",
    "        print()\n",
    "        print(\" SYNTHÈSE:\")\n",
    "        n_ok = len(psi_df[psi_df['Status'] == 'OK'])\n",
    "        n_warning = len(psi_df[psi_df['Status'] == 'WARNING'])\n",
    "        n_critical = len(psi_df[psi_df['Status'] == 'CRITICAL'])\n",
    "        \n",
    "        print(f\"  -  Stables (PSI < 0.10): {n_ok} features ({n_ok/len(psi_df)*100:.1f}%)\")\n",
    "        print(f\"  -  Drift modéré (0.10 ≤ PSI < 0.25): {n_warning} features ({n_warning/len(psi_df)*100:.1f}%)\")\n",
    "        print(f\"  -  Drift critique (PSI ≥ 0.25): {n_critical} features ({n_critical/len(psi_df)*100:.1f}%)\")\n",
    "        print()\n",
    "        \n",
    "        if n_critical > 0:\n",
    "            print(\" ALERTE: Drift critique détecté!\")\n",
    "            print(\"   Recommandation: RE-ENTRAÎNEMENT DU MODÈLE NÉCESSAIRE\")\n",
    "            print()\n",
    "            print(\"   Features critiques:\")\n",
    "            for idx, row in psi_df[psi_df['Status'] == 'CRITICAL'].iterrows():\n",
    "                print(f\"     - {row['Feature']}: PSI = {row['PSI']:.4f}\")\n",
    "        elif n_warning > 0:\n",
    "            print(\" ATTENTION: Drift modéré détecté\")\n",
    "            print(\"   Recommandation: Surveiller de près, re-entraînement dans 1-2 mois\")\n",
    "        else:\n",
    "            print(\" SITUATION NORMALE\")\n",
    "            print(\"   Le modèle reste valide, population stable\")\n",
    "        \n",
    "        # Visualisation PSI\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        colors = ['green' if s == 'OK' else ('orange' if s == 'WARNING' else 'red') \n",
    "                  for s in psi_df['Status']]\n",
    "        \n",
    "        plt.barh(range(min(20, len(psi_df))), psi_df['PSI'].head(20), color=colors[:20])\n",
    "        plt.yticks(range(min(20, len(psi_df))), psi_df['Feature'].head(20), fontsize=10)\n",
    "        plt.xlabel('PSI Value', fontsize=12, fontweight='bold')\n",
    "        plt.title('Population Stability Index (PSI) par Feature', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Ajouter lignes de seuil\n",
    "        plt.axvline(0.10, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Seuil Warning (0.10)')\n",
    "        plt.axvline(0.25, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Seuil Critical (0.25)')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except NameError:\n",
    "        print(\" best_X_train non disponible, impossible de calculer PSI\")\n",
    "        print(\"   Solution: Exécuter les cellules précédentes\")\n",
    "    \n",
    "else:\n",
    "    print(\"Impossible de calculer PSI sans modèle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f54b6",
   "metadata": {},
   "source": [
    "## F6. TESTS DE DISCRIMINATION ET ANALYSE DE BIAIS\n",
    "\n",
    "###  Obligation Légale: Fair Lending\n",
    "\n",
    "**Fair Lending Act** interdit la discrimination basée sur:\n",
    "- Race, couleur, origine nationale\n",
    "- Religion\n",
    "- Sexe/Genre\n",
    "- État civil\n",
    "- Âge (sauf si justifié)\n",
    "- Source de revenus (assistance publique)\n",
    "\n",
    "###  Test du Disparate Impact (Four-Fifths Rule)\n",
    "\n",
    "**Règle des 4/5ème:**\n",
    "\n",
    "```\n",
    "Ratio = Taux d'approbation groupe protégé / Taux d'approbation groupe référence\n",
    "```\n",
    "\n",
    "**Interprétation:**\n",
    "- Ratio ≥ 0.80 (80%):  Pas de disparate impact\n",
    "- Ratio < 0.80:  Possible discrimination, investigation requise\n",
    "\n",
    "**Exemple:**\n",
    "- Groupe A: 60% d'approbation\n",
    "- Groupe B: 40% d'approbation\n",
    "- Ratio: 40/60 = 0.67 < 0.80  (disparate impact détecté)\n",
    "\n",
    "###  Notre Analyse\n",
    "\n",
    "**Limitations du dataset:**\n",
    "- Pas de variables de genre, race, religion (bon pour la conformité !)\n",
    "- Variable `age` présente mais utilisée prudemment\n",
    "\n",
    "Nous allons tester:\n",
    "1. **Disparate impact par âge** (jeunes vs seniors)\n",
    "2. **Disparate impact par niveau de revenu** (proxy socio-économique)\n",
    "3. **Corrélations entre features** (discrimination indirecte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5df5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests de Discrimination\n",
    "if model_available:\n",
    "    print(\"=\"*60)\n",
    "    print(\"TESTS DE DISCRIMINATION - FOUR-FIFTHS RULE\")\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    \n",
    "    # Test 1: Disparate impact par âge\n",
    "    print(\"TEST 1: DISPARATE IMPACT PAR ÂGE\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Vérifier si 'age' existe dans les features\n",
    "    if 'age' in feature_names:\n",
    "        age_idx = feature_names.index('age')\n",
    "        ages = best_X_test[:, age_idx].values\n",
    "        \n",
    "        # Définir groupes (exemple: jeunes < 35 vs seniors >= 50)\n",
    "        # NOTE: Sur données standardisées, il faut reconvertir\n",
    "        # Pour simplifier, on crée des groupes basés sur les quartiles\n",
    "        age_q1 = np.percentile(ages, 25)\n",
    "        age_q3 = np.percentile(ages, 75)\n",
    "        \n",
    "        group_young = ages <= age_q1\n",
    "        group_senior = ages >= age_q3\n",
    "        \n",
    "        # Calculer taux d'approbation\n",
    "        y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "        \n",
    "        approval_rate_young = y_pred_optimal[group_young].mean()\n",
    "        approval_rate_senior = y_pred_optimal[group_senior].mean()\n",
    "        \n",
    "        ratio = min(approval_rate_young, approval_rate_senior) / max(approval_rate_young, approval_rate_senior)\n",
    "        \n",
    "        print(f\"Groupe 'Jeunes' (Q1): {group_young.sum()} personnes\")\n",
    "        print(f\"  Taux d'approbation: {approval_rate_young:.2%}\")\n",
    "        print()\n",
    "        print(f\"Groupe 'Seniors' (Q3): {group_senior.sum()} personnes\")\n",
    "        print(f\"  Taux d'approbation: {approval_rate_senior:.2%}\")\n",
    "        print()\n",
    "        print(f\"Ratio (Four-Fifths Rule): {ratio:.3f}\")\n",
    "        print()\n",
    "        \n",
    "        if ratio >= 0.80:\n",
    "            print(\" PAS de disparate impact détecté (ratio ≥ 0.80)\")\n",
    "        else:\n",
    "            print(f\" POSSIBLE DISPARATE IMPACT (ratio = {ratio:.3f} < 0.80)\")\n",
    "            print(\"   → Investigation requise\")\n",
    "            print(\"   → Justification métier nécessaire si variable âge utilisée\")\n",
    "    else:\n",
    "        print(\"ℹ Variable 'age' non trouvée dans les features\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    \n",
    "    # Test 2: Disparate impact par revenu\n",
    "    print(\"TEST 2: DISPARATE IMPACT PAR NIVEAU DE REVENU\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if 'annual_income' in feature_names:\n",
    "        income_idx = feature_names.index('annual_income')\n",
    "        incomes = best_X_test[:, income_idx].values\n",
    "        \n",
    "        # Groupes: bas revenus (Q1) vs hauts revenus (Q3)\n",
    "        income_q1 = np.percentile(incomes, 25)\n",
    "        income_q3 = np.percentile(incomes, 75)\n",
    "        \n",
    "        group_low_income = incomes <= income_q1\n",
    "        group_high_income = incomes >= income_q3\n",
    "        \n",
    "        approval_rate_low = y_pred_optimal[group_low_income].mean()\n",
    "        approval_rate_high = y_pred_optimal[group_high_income].mean()\n",
    "        \n",
    "        ratio_income = approval_rate_low / approval_rate_high if approval_rate_high > 0 else 0\n",
    "        \n",
    "        print(f\"Groupe 'Bas Revenus' (Q1): {group_low_income.sum()} personnes\")\n",
    "        print(f\"  Taux d'approbation: {approval_rate_low:.2%}\")\n",
    "        print()\n",
    "        print(f\"Groupe 'Hauts Revenus' (Q3): {group_high_income.sum()} personnes\")\n",
    "        print(f\"  Taux d'approbation: {approval_rate_high:.2%}\")\n",
    "        print()\n",
    "        print(f\"Ratio: {ratio_income:.3f}\")\n",
    "        print()\n",
    "        \n",
    "        if ratio_income >= 0.80:\n",
    "            print(\" PAS de disparate impact détecté (ratio ≥ 0.80)\")\n",
    "        else:\n",
    "            print(f\" POSSIBLE DISPARATE IMPACT (ratio = {ratio_income:.3f} < 0.80)\")\n",
    "            print()\n",
    "            print(\" NOTE: Il est NORMAL que les bas revenus aient un taux d'approbation plus faible.\")\n",
    "            print(\"   Ce n'est PAS une discrimination si basé sur capacité de remboursement objective.\")\n",
    "            print(\"   La discrimination serait si deux clients avec MÊME capacité étaient traités différemment.\")\n",
    "    else:\n",
    "        print(\"ℹ Variable 'annual_income' non trouvée\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "    \n",
    "    # Test 3: Analyse de corrélations (discrimination indirecte)\n",
    "    print(\"TEST 3: ANALYSE DE CORRÉLATIONS (Discrimination Indirecte)\")\n",
    "    print(\"-\" * 60)\n",
    "    print()\n",
    "    print(\" IMPORTANT: Même si variables protégées non utilisées directement,\")\n",
    "    print(\"   elles peuvent être corrélées avec d'autres features (discrimination indirecte).\")\n",
    "    print()\n",
    "    print(\"Exemples de proxies dangereux:\")\n",
    "    print(\"  - Code postal → Origine ethnique (redlining)\")\n",
    "    print(\"  - Prénom → Genre\")\n",
    "    print(\"  - Quartier → Niveau socio-économique\")\n",
    "    print()\n",
    "    print(\" Dans ce dataset:\")\n",
    "    print(\"   Pas de code postal, prénom, ou autres proxies évidents\")\n",
    "    print(\"   Variable 'age' présente: à justifier si utilisée\")\n",
    "    print()\n",
    "    print(\"Recommandation: Documenter l'absence de variables protégées et proxies.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Impossible de tester sans modèle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2z12k0utcmk",
   "metadata": {},
   "source": [
    "## F7. WoE ENCODING & INFORMATION VALUE\n",
    "\n",
    "###  Pourquoi WoE en Credit Scoring ?\n",
    "\n",
    "Le **Weight of Evidence (WoE)** est le **standard industrie** pour le credit scoring bancaire.\n",
    "\n",
    "###  Qu'est-ce que le WoE ?\n",
    "\n",
    "**WoE mesure la force prédictive** d'une catégorie:\n",
    "\n",
    "```\n",
    "WoE = ln(% Good / % Bad)\n",
    "```\n",
    "\n",
    "**Interprétation:**\n",
    "- **WoE > 0**: Plus de Good que de Bad → Risque FAIBLE\n",
    "- **WoE < 0**: Plus de Bad que de Good → Risque ÉLEVÉ  \n",
    "- **WoE = 0**: Équilibré → Risque NEUTRE\n",
    "\n",
    "###  Information Value (IV)\n",
    "\n",
    "L'**IV** mesure le **pouvoir discriminant total** d'une feature:\n",
    "\n",
    "```\n",
    "IV = Σ (% Good - % Bad) × WoE\n",
    "```\n",
    "\n",
    "**Seuils standard:**\n",
    "| IV | Interprétation | Utilisation |\n",
    "|----|----------------|-------------|\n",
    "| < 0.02 |  Pas de pouvoir prédictif | À exclure |\n",
    "| 0.02-0.10 |  Faible | À considérer |\n",
    "| 0.10-0.30 |  Moyen | **Utilisable** |\n",
    "| 0.30-0.50 |  Fort | **Excellent** |\n",
    "| > 0.50 |  Suspect | Vérifier overfitting |\n",
    "\n",
    "###  Avantages pour la Banque\n",
    "\n",
    "1.  **Transformation monotone** des features\n",
    "2.  **Gestion automatique** des valeurs manquantes  \n",
    "3.  **Réduction bruit** via binning\n",
    "4.  **Interprétabilité** pour auditeurs\n",
    "5.  **Standard industrie** (Basel II/III)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nb01u8k89qg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== F7: WoE ENCODING & INFORMATION VALUE ==========\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"F7. WoE ENCODING & INFORMATION VALUE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_woe_iv(feature, target, n_bins=10, min_samples=50):\n",
    "    \"\"\"Calcule WoE et IV pour une feature\"\"\"\n",
    "    df = pd.DataFrame({'feature': feature, 'target': target})\n",
    "    \n",
    "    # Binning\n",
    "    if pd.api.types.is_numeric_dtype(feature):\n",
    "        try:\n",
    "            df['bin'] = pd.qcut(feature, q=n_bins, duplicates='drop')\n",
    "        except:\n",
    "            df['bin'] = pd.cut(feature, bins=min(5, n_bins), duplicates='drop')\n",
    "    else:\n",
    "        df['bin'] = feature\n",
    "    \n",
    "    # Distribution Good/Bad par bin\n",
    "    grouped = df.groupby('bin')['target'].agg(['count', 'sum'])\n",
    "    grouped.columns = ['Total', 'Bad']\n",
    "    grouped['Good'] = grouped['Total'] - grouped['Bad']\n",
    "    grouped = grouped[grouped['Total'] >= min_samples]\n",
    "    \n",
    "    if len(grouped) == 0:\n",
    "        return None, 0\n",
    "    \n",
    "    total_good = grouped['Good'].sum()\n",
    "    total_bad = grouped['Bad'].sum()\n",
    "    \n",
    "    if total_good == 0 or total_bad == 0:\n",
    "        return None, 0\n",
    "    \n",
    "    grouped['%Good'] = grouped['Good'] / total_good\n",
    "    grouped['%Bad'] = grouped['Bad'] / total_bad\n",
    "    \n",
    "    # Éviter division par zéro\n",
    "    epsilon = 0.0001\n",
    "    grouped['%Good'] = grouped['%Good'].replace(0, epsilon)\n",
    "    grouped['%Bad'] = grouped['%Bad'].replace(0, epsilon)\n",
    "    \n",
    "    # WoE et IV\n",
    "    grouped['WoE'] = np.log(grouped['%Good'] / grouped['%Bad'])\n",
    "    grouped['IV'] = (grouped['%Good'] - grouped['%Bad']) * grouped['WoE']\n",
    "    iv = grouped['IV'].sum()\n",
    "    \n",
    "    return grouped.reset_index(), iv\n",
    "\n",
    "if 'best_X_train' in locals() and 'best_y_train' in locals():\n",
    "    print(\"\\n ANALYSE WoE/IV DES FEATURES\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Convertir en DataFrame\n",
    "    if isinstance(best_X_train, np.ndarray):\n",
    "        if 'X_train_v2' in locals():\n",
    "            feature_names = X_train_v2.columns.tolist()\n",
    "        elif 'X_train' in locals() and hasattr(X_train, 'columns'):\n",
    "            feature_names = X_train.columns.tolist()\n",
    "        else:\n",
    "            feature_names = [f'feature_{i}' for i in range(best_X_train.shape[1])]\n",
    "        X_train_df = pd.DataFrame(best_X_train, columns=feature_names)\n",
    "    else:\n",
    "        X_train_df = best_X_train.copy()\n",
    "        feature_names = X_train_df.columns.tolist()\n",
    "    \n",
    "    print(f\"   Features: {len(feature_names)}, Samples: {len(best_y_train):,}\")\n",
    "    \n",
    "    # Calculer IV\n",
    "    print(f\"\\n CALCUL INFORMATION VALUE...\")\n",
    "    iv_results = []\n",
    "    woe_data = {}\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        woe_df, iv = calculate_woe_iv(X_train_df[feature], best_y_train, n_bins=10, min_samples=50)\n",
    "        \n",
    "        if woe_df is not None:\n",
    "            woe_data[feature] = woe_df\n",
    "            \n",
    "            if iv < 0.02:\n",
    "                interpretation = \" Pas de pouvoir\"\n",
    "            elif iv < 0.10:\n",
    "                interpretation = \" Faible\"\n",
    "            elif iv < 0.30:\n",
    "                interpretation = \" Moyen\"\n",
    "            elif iv < 0.50:\n",
    "                interpretation = \" Fort\"\n",
    "            else:\n",
    "                interpretation = \" Suspect\"\n",
    "            \n",
    "            iv_results.append({'Feature': feature, 'IV': iv, 'Interpretation': interpretation})\n",
    "    \n",
    "    iv_df = pd.DataFrame(iv_results).sort_values('IV', ascending=False)\n",
    "    \n",
    "    # Afficher rapport\n",
    "    print(f\"\\n RAPPORT INFORMATION VALUE (Top 20)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Feature':<40} {'IV':>10} {'Interprétation':>20}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for idx, row in iv_df.head(20).iterrows():\n",
    "        print(f\"{row['Feature']:<40} {row['IV']:>10.4f} {row['Interpretation']:>20}\")\n",
    "    \n",
    "    # Statistiques\n",
    "    print(f\"\\n STATISTIQUES:\")\n",
    "    print(f\"   Total features: {len(iv_df)}\")\n",
    "    print(f\"    Pas de pouvoir (IV < 0.02): {len(iv_df[iv_df['IV'] < 0.02])}\")\n",
    "    print(f\"    Faible (0.02 ≤ IV < 0.10): {len(iv_df[(iv_df['IV'] >= 0.02) & (iv_df['IV'] < 0.10)])}\")\n",
    "    print(f\"    Moyen (0.10 ≤ IV < 0.30): {len(iv_df[(iv_df['IV'] >= 0.10) & (iv_df['IV'] < 0.30)])}\")\n",
    "    print(f\"    Fort (0.30 ≤ IV < 0.50): {len(iv_df[(iv_df['IV'] >= 0.30) & (iv_df['IV'] < 0.50)])}\")\n",
    "    print(f\"    Suspect (IV ≥ 0.50): {len(iv_df[iv_df['IV'] >= 0.50])}\")\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Top 10 IV\n",
    "    ax1 = axes[0]\n",
    "    top_10 = iv_df.head(10)\n",
    "    colors = ['green' if iv >= 0.30 else 'steelblue' if iv >= 0.10 else 'orange' if iv >= 0.02 else 'red' for iv in top_10['IV']]\n",
    "    \n",
    "    ax1.barh(range(len(top_10)), top_10['IV'], color=colors, edgecolor='black')\n",
    "    ax1.set_yticks(range(len(top_10)))\n",
    "    ax1.set_yticklabels(top_10['Feature'], fontsize=10)\n",
    "    ax1.set_xlabel('Information Value (IV)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Top 10 Features by IV', fontsize=14, fontweight='bold')\n",
    "    ax1.axvline(x=0.30, color='green', linestyle='--', linewidth=1.5, alpha=0.7, label='Fort (0.30)')\n",
    "    ax1.axvline(x=0.10, color='steelblue', linestyle='--', linewidth=1.5, alpha=0.7, label='Moyen (0.10)')\n",
    "    ax1.axvline(x=0.02, color='orange', linestyle='--', linewidth=1.5, alpha=0.7, label='Faible (0.02)')\n",
    "    ax1.legend(loc='best', fontsize=9)\n",
    "    ax1.grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot 2: Distribution IV\n",
    "    ax2 = axes[1]\n",
    "    iv_bins = [0, 0.02, 0.10, 0.30, 0.50, 1.0]\n",
    "    iv_labels = ['No Power\\n(<0.02)', 'Weak\\n(0.02-0.10)', 'Medium\\n(0.10-0.30)', 'Strong\\n(0.30-0.50)', 'Suspect\\n(>0.50)']\n",
    "    iv_counts = pd.cut(iv_df['IV'], bins=iv_bins, labels=iv_labels).value_counts()\n",
    "    \n",
    "    colors_dist = ['red', 'orange', 'steelblue', 'green', 'purple']\n",
    "    ax2.bar(range(len(iv_counts)), iv_counts.values, color=colors_dist, edgecolor='black')\n",
    "    ax2.set_xticks(range(len(iv_counts)))\n",
    "    ax2.set_xticklabels(iv_counts.index, fontsize=10)\n",
    "    ax2.set_ylabel('Number of Features', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Distribution of IV Scores', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, v in enumerate(iv_counts.values):\n",
    "        ax2.text(i, v + 0.3, str(v), ha='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"f7_information_value_analysis.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n GRAPHIQUE SAUVEGARDÉ: f7_information_value_analysis.png\")\n",
    "    \n",
    "    # Recommandations\n",
    "    print(f\"\\n RECOMMANDATIONS:\")\n",
    "    print(\"=\"*80)\n",
    "    features_to_keep = iv_df[iv_df['IV'] >= 0.10]['Feature'].tolist()\n",
    "    features_to_drop = iv_df[iv_df['IV'] < 0.02]['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\n FEATURES À CONSERVER (IV ≥ 0.10): {len(features_to_keep)}\")\n",
    "    print(f\" FEATURES À EXCLURE (IV < 0.02): {len(features_to_drop)}\")\n",
    "    \n",
    "    print(f\"\\n NEXT STEPS:\")\n",
    "    print(\"   1. Implémenter WoE encoding complet\")\n",
    "    print(\"   2. Ré-entraîner modèle avec features WoE (IV ≥ 0.10)\")\n",
    "    print(\"   3. Documenter choix de features pour audit\")\n",
    "    \n",
    "    iv_df.to_csv(os.path.join(OUTPUT_DIR, \"f7_information_value_report.csv\"), index=False)\n",
    "    print(f\"\\n RAPPORT SAUVEGARDÉ: f7_information_value_report.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" ANALYSE WoE/IV TERMINÉE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "else:\n",
    "    print(\"\\n ERREUR: Données d'entraînement non disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d0ba0",
   "metadata": {},
   "source": [
    "## F8. VIF - DÉTECTION DE MULTICOLLINÉARITÉ\n",
    "\n",
    "###  Pourquoi tester la multicollinéarité ?\n",
    "\n",
    "La **multicollinéarité** survient quand des features sont **fortement corrélées**.\n",
    "\n",
    "**Problèmes causés:**\n",
    "-  Coefficients instables (varient énormément entre entraînements)\n",
    "-  Interprétation impossible (quel feature a vraiment l'impact?)\n",
    "-  Overfitting potentiel\n",
    "-  Non-conformité audit (modèle non-explicable)\n",
    "\n",
    "###  Variance Inflation Factor (VIF)\n",
    "\n",
    "Le **VIF** mesure à quel point une variable est expliquée par les autres:\n",
    "\n",
    "```\n",
    "VIF = 1 / (1 - R²)\n",
    "```\n",
    "\n",
    "Où R² est le coefficient de détermination en régressant la feature sur toutes les autres.\n",
    "\n",
    "**Interprétation:**\n",
    "| VIF | Corrélation | Action |\n",
    "|-----|-------------|--------|\n",
    "| 1 |  Aucune | Parfait |\n",
    "| 1-5 |  Modérée | Acceptable |\n",
    "| 5-10 |  Élevée | À surveiller |\n",
    "| > 10 |  CRITIQUE | **À RETIRER** |\n",
    "\n",
    "###  Standard Bancaire\n",
    "\n",
    "En credit scoring professionnel:\n",
    "- **VIF > 10**: Feature OBLIGATOIREMENT retirée\n",
    "- **VIF 5-10**: Review par Risk Committee\n",
    "- **Rapport VIF**: Document d'audit obligatoire\n",
    "\n",
    "###  Solutions\n",
    "\n",
    "Si VIF élevé détecté:\n",
    "1. Retirer une des features corrélées (garder celle avec IV plus élevé)\n",
    "2. Créer feature composite (PCA, moyenne pondérée)\n",
    "3. Régularisation L1 (Lasso) qui sélectionne automatiquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w1eali5riye",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== F8: VIF - DÉTECTION MULTICOLLINÉARITÉ ==========\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"F8. VIF - DÉTECTION DE MULTICOLLINÉARITÉ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_vif(X_df, threshold=10.0):\n",
    "    \"\"\"Calcule VIF pour chaque feature\"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    vif_data = []\n",
    "    features = X_df.columns.tolist()\n",
    "    \n",
    "    print(f\"   Calcul VIF pour {len(features)} features...\")\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"      Progress: {i+1}/{len(features)}\")\n",
    "        \n",
    "        X_others = X_df.drop(columns=[feature])\n",
    "        y_target = X_df[feature]\n",
    "        \n",
    "        try:\n",
    "            lr = LinearRegression()\n",
    "            lr.fit(X_others, y_target)\n",
    "            r_squared = lr.score(X_others, y_target)\n",
    "            \n",
    "            vif = 1 / (1 - r_squared) if r_squared < 0.9999 else np.inf\n",
    "            \n",
    "            if vif > threshold:\n",
    "                status, action = \" CRITIQUE\", \"À RETIRER\"\n",
    "            elif vif > 5:\n",
    "                status, action = \" ÉLEVÉ\", \"À SURVEILLER\"\n",
    "            elif vif > 1:\n",
    "                status, action = \" MODÉRÉ\", \"OK\"\n",
    "            else:\n",
    "                status, action = \" FAIBLE\", \"Excellent\"\n",
    "            \n",
    "            vif_data.append({'Feature': feature, 'VIF': vif, 'Status': status, 'Action': action})\n",
    "        except:\n",
    "            vif_data.append({'Feature': feature, 'VIF': np.nan, 'Status': ' ERREUR', 'Action': 'Vérifier'})\n",
    "    \n",
    "    return pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n",
    "\n",
    "if 'best_X_train' in locals():\n",
    "    print(\"\\n PRÉPARATION DES DONNÉES\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if isinstance(best_X_train, np.ndarray):\n",
    "        if 'X_train_v2' in locals():\n",
    "            feature_names = X_train_v2.columns.tolist()\n",
    "        elif 'X_train' in locals() and hasattr(X_train, 'columns'):\n",
    "            feature_names = X_train.columns.tolist()\n",
    "        else:\n",
    "            feature_names = [f'feature_{i}' for i in range(best_X_train.shape[1])]\n",
    "        X_train_df = pd.DataFrame(best_X_train, columns=feature_names)\n",
    "    else:\n",
    "        X_train_df = best_X_train.copy()\n",
    "        feature_names = X_train_df.columns.tolist()\n",
    "    \n",
    "    print(f\"   Features: {len(feature_names)}, Samples: {X_train_df.shape[0]:,}\")\n",
    "    \n",
    "    print(f\"\\n CALCUL DES VIF...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    vif_df = calculate_vif(X_train_df, threshold=10.0)\n",
    "    \n",
    "    print(f\"\\n CALCUL TERMINÉ\")\n",
    "    \n",
    "    # Afficher résultats\n",
    "    print(f\"\\n RAPPORT VIF (Top 20)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Feature':<40} {'VIF':>12} {'Status':>15} {'Action':>15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for idx, row in vif_df.head(20).iterrows():\n",
    "        vif_val = row['VIF']\n",
    "        vif_str = \"∞\" if np.isinf(vif_val) else \"N/A\" if np.isnan(vif_val) else f\"{vif_val:.2f}\"\n",
    "        print(f\"{row['Feature']:<40} {vif_str:>12} {row['Status']:>15} {row['Action']:>15}\")\n",
    "    \n",
    "    # Statistiques\n",
    "    print(f\"\\n STATISTIQUES:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    vif_df_clean = vif_df[~vif_df['VIF'].isna() & ~np.isinf(vif_df['VIF'])]\n",
    "    \n",
    "    n_critical = len(vif_df_clean[vif_df_clean['VIF'] > 10])\n",
    "    n_high = len(vif_df_clean[(vif_df_clean['VIF'] > 5) & (vif_df_clean['VIF'] <= 10)])\n",
    "    n_moderate = len(vif_df_clean[(vif_df_clean['VIF'] > 1) & (vif_df_clean['VIF'] <= 5)])\n",
    "    n_low = len(vif_df_clean[vif_df_clean['VIF'] <= 1])\n",
    "    \n",
    "    print(f\"   Total features analysées: {len(vif_df_clean)}\")\n",
    "    print(f\"    CRITIQUE (VIF > 10): {n_critical} features\")\n",
    "    print(f\"    ÉLEVÉ (5 < VIF ≤ 10): {n_high} features\")\n",
    "    print(f\"    MODÉRÉ (1 < VIF ≤ 5): {n_moderate} features\")\n",
    "    print(f\"    FAIBLE (VIF ≤ 1): {n_low} features\")\n",
    "    \n",
    "    if n_critical > 0:\n",
    "        print(f\"\\n ALERTE: {n_critical} features avec multicollinéarité CRITIQUE!\")\n",
    "        critical_features = vif_df_clean[vif_df_clean['VIF'] > 10]['Feature'].tolist()\n",
    "        for feat in critical_features[:10]:\n",
    "            print(f\"      - {feat}\")\n",
    "    \n",
    "    # Visualisation\n",
    "    print(f\"\\n VISUALISATION VIF\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: VIF scores (Top 15)\n",
    "    ax1 = axes[0]\n",
    "    top_15 = vif_df_clean.head(15)\n",
    "    colors_vif = ['red' if vif > 10 else 'orange' if vif > 5 else 'steelblue' if vif > 1 else 'green' for vif in top_15['VIF']]\n",
    "    \n",
    "    ax1.barh(range(len(top_15)), top_15['VIF'], color=colors_vif, edgecolor='black')\n",
    "    ax1.set_yticks(range(len(top_15)))\n",
    "    ax1.set_yticklabels(top_15['Feature'], fontsize=10)\n",
    "    ax1.set_xlabel('VIF Score', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Top 15 Features by VIF\\n(Higher = More Multicollinearity)', fontsize=14, fontweight='bold')\n",
    "    ax1.axvline(x=10, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Critical (10)')\n",
    "    ax1.axvline(x=5, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='High (5)')\n",
    "    ax1.axvline(x=1, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Low (1)')\n",
    "    ax1.legend(loc='best', fontsize=10)\n",
    "    ax1.grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot 2: Distribution VIF\n",
    "    ax2 = axes[1]\n",
    "    vif_categories = pd.cut(vif_df_clean['VIF'], bins=[0, 1, 5, 10, 100], labels=['Low\\n(≤1)', 'Moderate\\n(1-5)', 'High\\n(5-10)', 'Critical\\n(>10)'])\n",
    "    vif_counts = vif_categories.value_counts()\n",
    "    \n",
    "    colors_dist = ['green', 'steelblue', 'orange', 'red']\n",
    "    ax2.bar(range(len(vif_counts)), vif_counts.values, color=colors_dist, edgecolor='black')\n",
    "    ax2.set_xticks(range(len(vif_counts)))\n",
    "    ax2.set_xticklabels(vif_counts.index, fontsize=11)\n",
    "    ax2.set_ylabel('Number of Features', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Distribution of VIF Scores', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, v in enumerate(vif_counts.values):\n",
    "        ax2.text(i, v + 0.5, str(v), ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"f8_vif_analysis.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n GRAPHIQUE SAUVEGARDÉ: f8_vif_analysis.png\")\n",
    "    \n",
    "    # Recommandations\n",
    "    print(f\"\\n RECOMMANDATIONS:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if n_critical > 0:\n",
    "        print(f\"\\n ACTIONS URGENTES:\")\n",
    "        print(f\"   {n_critical} features avec multicollinéarité CRITIQUE\")\n",
    "        print(\"   Option 1: RETIRER les features (garder celle avec IV le plus élevé)\")\n",
    "        print(\"   Option 2: COMBINER les features (PCA, moyenne)\")\n",
    "        print(\"   Option 3: RÉGULARISATION L1 (Lasso)\")\n",
    "    \n",
    "    if n_high > 0:\n",
    "        print(f\"\\n À SURVEILLER (5 < VIF ≤ 10): {n_high} features\")\n",
    "        print(\"   → Review par Risk Committee recommandée\")\n",
    "    \n",
    "    if n_critical == 0 and n_high == 0:\n",
    "        print(f\"\\n AUCUN PROBLÈME DÉTECTÉ\")\n",
    "        print(\"   → Multicollinéarité acceptable\")\n",
    "        print(\"   → Modèle conforme pour production\")\n",
    "    \n",
    "    vif_df.to_csv(os.path.join(OUTPUT_DIR, \"f8_vif_report.csv\"), index=False)\n",
    "    print(f\"\\n RAPPORT SAUVEGARDÉ: f8_vif_report.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" ANALYSE VIF TERMINÉE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "else:\n",
    "    print(\"\\n ERREUR: Données non disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r27q92afynr",
   "metadata": {},
   "source": [
    "## F9. TESTS DE MONOTONIE\n",
    "\n",
    "###  Pourquoi vérifier la monotonie ?\n",
    "\n",
    "En credit scoring, on s'attend à des **relations monotones**:\n",
    "- **Credit score ↑** → Probabilité défaut ↓\n",
    "- **Dette ↑** → Probabilité défaut ↑  \n",
    "\n",
    "Une **relation non-monotone** indique:\n",
    "-  Bruit dans les données\n",
    "-  Feature engineering nécessaire\n",
    "-  Problème de qualité des données\n",
    "\n",
    "###  Test de Monotonie\n",
    "\n",
    "On analyse la relation feature → target en créant des bins et en vérifiant si le taux de défaut évolue de manière monotone.\n",
    "\n",
    "**Corrélation de Spearman:**\n",
    "- |ρ| > 0.7: Relation monotone forte\n",
    "- 0.5 < |ρ| ≤ 0.7: Quasi-monotone\n",
    "- |ρ| ≤ 0.5: Non-monotone\n",
    "\n",
    "###  Importance\n",
    "\n",
    "-  Features monotones: Interprétables et fiables\n",
    "-  Features non-monotones: Nécessitent transformation ou binning\n",
    "-  Standard bancaire: Relations monotones préférées pour audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r4xp38yo4w",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== F9: TESTS DE MONOTONIE ==========\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"F9. TESTS DE MONOTONIE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def check_monotonicity(feature, target, n_bins=10):\n",
    "    \"\"\"Vérifie la monotonie de la relation feature-target\"\"\"\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    df_temp = pd.DataFrame({'feature': feature, 'target': target})\n",
    "    \n",
    "    try:\n",
    "        df_temp['bin'] = pd.qcut(df_temp['feature'], q=n_bins, duplicates='drop')\n",
    "    except:\n",
    "        df_temp['bin'] = pd.cut(df_temp['feature'], bins=5, duplicates='drop')\n",
    "    \n",
    "    grouped = df_temp.groupby('bin')['target'].mean().reset_index()\n",
    "    grouped = grouped.sort_values('bin')\n",
    "    default_rates = grouped['target'].values\n",
    "    \n",
    "    # Monotone croissante/décroissante\n",
    "    mono_increasing = np.all(np.diff(default_rates) >= 0)\n",
    "    mono_decreasing = np.all(np.diff(default_rates) <= 0)\n",
    "    \n",
    "    # Corrélation de Spearman\n",
    "    spearman_corr, _ = spearmanr(range(len(default_rates)), default_rates)\n",
    "    \n",
    "    if mono_increasing:\n",
    "        trend, status = \"↗ Monotone croissante\", \" MONOTONE\"\n",
    "    elif mono_decreasing:\n",
    "        trend, status = \"↘ Monotone décroissante\", \" MONOTONE\"\n",
    "    elif abs(spearman_corr) > 0.7:\n",
    "        trend = f\"~{'↗' if spearman_corr > 0 else '↘'} Quasi-monotone ({spearman_corr:.2f})\"\n",
    "        status = \" QUASI-MONOTONE\"\n",
    "    else:\n",
    "        trend, status = \" Non-monotone\", \" NON-MONOTONE\"\n",
    "    \n",
    "    return spearman_corr, trend, status\n",
    "\n",
    "if 'best_X_train' in locals() and 'best_y_train' in locals():\n",
    "    print(\"\\n ANALYSE DE MONOTONIE\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if isinstance(best_X_train, np.ndarray):\n",
    "        if 'X_train_v2' in locals():\n",
    "            feature_names = X_train_v2.columns.tolist()\n",
    "        elif 'X_train' in locals() and hasattr(X_train, 'columns'):\n",
    "            feature_names = X_train.columns.tolist()\n",
    "        else:\n",
    "            feature_names = [f'feature_{i}' for i in range(best_X_train.shape[1])]\n",
    "        X_train_df = pd.DataFrame(best_X_train, columns=feature_names)\n",
    "    else:\n",
    "        X_train_df = best_X_train.copy()\n",
    "        feature_names = X_train_df.columns.tolist()\n",
    "    \n",
    "    print(f\"   Features: {len(feature_names)}\")\n",
    "    \n",
    "    mono_results = []\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        if not pd.api.types.is_numeric_dtype(X_train_df[feature]):\n",
    "            continue\n",
    "        \n",
    "        spearman_corr, trend, status = check_monotonicity(X_train_df[feature], best_y_train)\n",
    "        mono_results.append({'Feature': feature, 'Spearman_Corr': spearman_corr, 'Trend': trend, 'Status': status})\n",
    "    \n",
    "    mono_df = pd.DataFrame(mono_results).sort_values('Spearman_Corr', key=abs, ascending=False)\n",
    "    \n",
    "    print(f\"\\n RÉSULTATS MONOTONIE (Top 20)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Feature':<40} {'Spearman':>10} {'Status':>20}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for idx, row in mono_df.head(20).iterrows():\n",
    "        print(f\"{row['Feature']:<40} {row['Spearman_Corr']:>10.3f} {row['Status']:>20}\")\n",
    "    \n",
    "    # Statistiques\n",
    "    n_mono = len(mono_df[mono_df['Status'] == \" MONOTONE\"])\n",
    "    n_quasi = len(mono_df[mono_df['Status'] == \" QUASI-MONOTONE\"])\n",
    "    n_non = len(mono_df[mono_df['Status'] == \" NON-MONOTONE\"])\n",
    "    \n",
    "    print(f\"\\n STATISTIQUES:\")\n",
    "    print(f\"    Monotones: {n_mono} ({n_mono/len(mono_df)*100:.1f}%)\")\n",
    "    print(f\"    Quasi-monotones: {n_quasi} ({n_quasi/len(mono_df)*100:.1f}%)\")\n",
    "    print(f\"    Non-monotones: {n_non} ({n_non/len(mono_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualisation\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    colors = ['green' if s == \" MONOTONE\" else 'orange' if s == \" QUASI-MONOTONE\" else 'red' for s in mono_df.head(15)['Status']]\n",
    "    \n",
    "    ax.barh(range(len(mono_df.head(15))), abs(mono_df.head(15)['Spearman_Corr']), color=colors, edgecolor='black')\n",
    "    ax.set_yticks(range(len(mono_df.head(15))))\n",
    "    ax.set_yticklabels(mono_df.head(15)['Feature'], fontsize=10)\n",
    "    ax.set_xlabel('|Spearman Correlation|', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Monotonicity Analysis - Top 15 Features', fontsize=14, fontweight='bold')\n",
    "    ax.axvline(x=0.7, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Strong Monotone (0.7)')\n",
    "    ax.axvline(x=0.5, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Quasi-Monotone (0.5)')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"f9_monotonicity_analysis.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n GRAPHIQUE SAUVEGARDÉ: f9_monotonicity_analysis.png\")\n",
    "    \n",
    "    print(f\"\\n RECOMMANDATIONS:\")\n",
    "    if n_non > 0:\n",
    "        print(f\"    {n_non} features non-monotones détectées\")\n",
    "        print(\"   → Appliquer WoE encoding (F7) pour forcer monotonie\")\n",
    "        print(\"   → Ou créer bins personnalisés\")\n",
    "    else:\n",
    "        print(\"    Toutes les features sont monotones\")\n",
    "        print(\"   → Relations fiables pour credit scoring\")\n",
    "    \n",
    "    mono_df.to_csv(os.path.join(OUTPUT_DIR, \"f9_monotonicity_report.csv\"), index=False)\n",
    "    print(f\"\\n RAPPORT SAUVEGARDÉ: f9_monotonicity_report.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" ANALYSE MONOTONIE TERMINÉE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "else:\n",
    "    print(\"\\n ERREUR: Données non disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qt9tuvor8no",
   "metadata": {},
   "source": [
    "## F10. SHAP VALUES - INTERPRÉTABILITÉ RÉGLEMENTAIRE\n",
    "\n",
    "###  Pourquoi SHAP est OBLIGATOIRE ?\n",
    "\n",
    "**RGPD Article 22** exige le **droit à l'explication** pour toute décision automatisée.\n",
    "\n",
    "Sans SHAP/LIME, vous **NE POUVEZ PAS**:\n",
    "-  Justifier un refus de prêt à un client\n",
    "-  Passer un audit compliance\n",
    "-  Déployer en production bancaire\n",
    "\n",
    "###  Qu'est-ce que SHAP ?\n",
    "\n",
    "**SHapley Additive exPlanations** décompose chaque prédiction en contributions des features.\n",
    "\n",
    "**Exemple:**\n",
    "```\n",
    "Client X: Probabilité défaut = 0.35 (35%)\n",
    "\n",
    "Contributions:\n",
    "  Credit score (650):       -0.15  (réduit risque)\n",
    "  Dette/revenu (0.45):      +0.08  (augmente risque)\n",
    "  Revenu annuel (45K):      -0.03  (réduit risque)\n",
    "  Age (28):                 +0.05  (augmente risque)\n",
    "  Baseline:                 +0.40\n",
    "                            -----\n",
    "  Total:                    0.35 \n",
    "```\n",
    "\n",
    "###  Utilisation Bancaire\n",
    "\n",
    "**Pour chaque refus de prêt, fournir:**\n",
    "1. Top 3 raisons du refus (features avec contributions négatives)\n",
    "2. Explication claire en langage naturel\n",
    "3. Actions possibles pour le client\n",
    "\n",
    "###  Conformité Légale\n",
    "\n",
    "-  **Fair Lending Act**: Démontrer absence de biais\n",
    "-  **RGPD**: Droit à l'explication respecté\n",
    "-  **Audit**: Traçabilité complète"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m8kfmyb9xyp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== F10: SHAP VALUES - INTERPRÉTABILITÉ RÉGLEMENTAIRE ==========\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"F10. SHAP VALUES - EXPLICATION DES PRÉDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Installation de shap si nécessaire\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    print(\"\\n SHAP déjà installé\")\n",
    "except ImportError:\n",
    "    print(\"\\n Installation de SHAP...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"shap\"])\n",
    "    import shap\n",
    "    print(\" SHAP installé avec succès\")\n",
    "\n",
    "if 'best_model' in locals() and best_model is not None:\n",
    "    print(\"\\n CALCUL DES SHAP VALUES\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Échantillonner pour performance (SHAP peut être lent)\n",
    "    n_background = min(100, len(best_X_train))\n",
    "    n_explain = min(100, len(best_X_test))\n",
    "    \n",
    "    print(f\"   Background samples: {n_background}\")\n",
    "    print(f\"   Samples à expliquer: {n_explain}\")\n",
    "    \n",
    "    # Créer explainer\n",
    "    print(\"\\n Création de l'explainer SHAP...\")\n",
    "    \n",
    "    # Pour modèle linéaire, utiliser LinearExplainer (plus rapide)\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    if isinstance(best_model, LogisticRegression):\n",
    "        # Background data\n",
    "        if isinstance(best_X_train, np.ndarray):\n",
    "            X_background = best_X_train[:n_background]\n",
    "        else:\n",
    "            X_background = best_X_train.iloc[:n_background].values\n",
    "        \n",
    "        explainer = shap.LinearExplainer(best_model, X_background)\n",
    "        print(\"   Type: LinearExplainer (rapide)\")\n",
    "    else:\n",
    "        # KernelExplainer pour autres modèles\n",
    "        if isinstance(best_X_train, np.ndarray):\n",
    "            X_background = best_X_train[:n_background]\n",
    "        else:\n",
    "            X_background = best_X_train.iloc[:n_background].values\n",
    "        \n",
    "        explainer = shap.KernelExplainer(\n",
    "            lambda x: best_model.predict_proba(x)[:, 1],\n",
    "            X_background\n",
    "        )\n",
    "        print(\"   Type: KernelExplainer\")\n",
    "    \n",
    "    # Calculer SHAP values\n",
    "    print(\"\\n Calcul des SHAP values (peut prendre quelques secondes)...\")\n",
    "    \n",
    "    if isinstance(best_X_test, np.ndarray):\n",
    "        X_explain = best_X_test[:n_explain]\n",
    "    else:\n",
    "        X_explain = best_X_test.iloc[:n_explain].values\n",
    "    \n",
    "    shap_values = explainer.shap_values(X_explain)\n",
    "    \n",
    "    print(\" SHAP values calculées\")\n",
    "    \n",
    "    # Get feature names\n",
    "    if 'X_train_v2' in locals():\n",
    "        feature_names = X_train_v2.columns.tolist()\n",
    "    elif 'X_train' in locals() and hasattr(X_train, 'columns'):\n",
    "        feature_names = X_train.columns.tolist()\n",
    "    else:\n",
    "        feature_names = [f'feature_{i}' for i in range(X_explain.shape[1])]\n",
    "    \n",
    "    # Visualisation 1: Summary Plot\n",
    "    print(\"\\n VISUALISATION 1: SUMMARY PLOT\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_explain, feature_names=feature_names, show=False, max_display=15)\n",
    "    plt.title('SHAP Summary Plot\\nImpact des Features sur les Prédictions', fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"f10_shap_summary_plot.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\" GRAPHIQUE SAUVEGARDÉ: f10_shap_summary_plot.png\")\n",
    "    \n",
    "    # Visualisation 2: Feature Importance\n",
    "    print(\"\\n VISUALISATION 2: FEATURE IMPORTANCE\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_explain, feature_names=feature_names, plot_type=\"bar\", show=False, max_display=15)\n",
    "    plt.title('SHAP Feature Importance\\n(Mean |SHAP value|)', fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"f10_shap_feature_importance.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\" GRAPHIQUE SAUVEGARDÉ: f10_shap_feature_importance.png\")\n",
    "    \n",
    "    # Exemple d'explication individuelle\n",
    "    print(\"\\n VISUALISATION 3: EXPLICATION INDIVIDUELLE (Waterfall)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Prendre un exemple de refus (prédiction positive = défaut)\n",
    "    y_pred_proba_sample = best_model.predict_proba(X_explain)[:, 1]\n",
    "    \n",
    "    # Trouver un exemple avec forte probabilité de défaut\n",
    "    idx_high_risk = np.argmax(y_pred_proba_sample)\n",
    "    \n",
    "    print(f\"   Client sélectionné (index {idx_high_risk}):\")\n",
    "    print(f\"   Probabilité de défaut: {y_pred_proba_sample[idx_high_risk]*100:.1f}%\")\n",
    "    \n",
    "    # Créer waterfall plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Créer Explanation object pour SHAP v0.40+\n",
    "    if hasattr(shap, 'Explanation'):\n",
    "        explanation = shap.Explanation(\n",
    "            values=shap_values[idx_high_risk],\n",
    "            base_values=explainer.expected_value if hasattr(explainer, 'expected_value') else 0,\n",
    "            data=X_explain[idx_high_risk],\n",
    "            feature_names=feature_names\n",
    "        )\n",
    "        shap.plots.waterfall(explanation, show=False, max_display=10)\n",
    "    else:\n",
    "        # Fallback pour anciennes versions\n",
    "        shap.waterfall_plot(\n",
    "            explainer.expected_value if hasattr(explainer, 'expected_value') else 0,\n",
    "            shap_values[idx_high_risk],\n",
    "            X_explain[idx_high_risk],\n",
    "            feature_names=feature_names,\n",
    "            show=False,\n",
    "            max_display=10\n",
    "        )\n",
    "    \n",
    "    plt.title(f'Explication Individuelle - Client à Haut Risque\\nProba Défaut = {y_pred_proba_sample[idx_high_risk]*100:.1f}%',\n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"f10_shap_waterfall_example.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\" GRAPHIQUE SAUVEGARDÉ: f10_shap_waterfall_example.png\")\n",
    "    \n",
    "    # Générer explication textuelle\n",
    "    print(\"\\n EXPLICATION TEXTUELLE POUR LE CLIENT:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Top 3 contributions\n",
    "    shap_vals_sample = shap_values[idx_high_risk]\n",
    "    top_3_idx = np.argsort(np.abs(shap_vals_sample))[-3:][::-1]\n",
    "    \n",
    "    print(f\"\\n DÉCISION: PRÊT REFUSÉ\")\n",
    "    print(f\"   Probabilité de défaut: {y_pred_proba_sample[idx_high_risk]*100:.1f}% (seuil: 50%)\")\n",
    "    print(f\"\\n PRINCIPALES RAISONS DU REFUS:\")\n",
    "    \n",
    "    for i, idx in enumerate(top_3_idx, 1):\n",
    "        feature = feature_names[idx]\n",
    "        shap_val = shap_vals_sample[idx]\n",
    "        feature_val = X_explain[idx_high_risk, idx]\n",
    "        \n",
    "        impact = \"AUGMENTE\" if shap_val > 0 else \"RÉDUIT\"\n",
    "        direction = \"\" if shap_val > 0 else \"\"\n",
    "        \n",
    "        print(f\"\\n   {i}. {feature}\")\n",
    "        print(f\"      Valeur: {feature_val:.3f}\")\n",
    "        print(f\"      {direction} Impact: {impact} le risque de {abs(shap_val)*100:.1f} points de %\")\n",
    "    \n",
    "    print(f\"\\n ACTIONS RECOMMANDÉES POUR LE CLIENT:\")\n",
    "    print(\"   1. Améliorer le credit score (paiements à temps)\")\n",
    "    print(\"   2. Réduire le ratio dette/revenu\")\n",
    "    print(\"   3. Augmenter l'ancienneté emploi/adresse\")\n",
    "    \n",
    "    print(\"\\n DROIT DE RECOURS:\")\n",
    "    print(\"   Le client peut contester cette décision en contactant le service client.\")\n",
    "    \n",
    "    # Sauvegarder SHAP values pour usage ultérieur\n",
    "    np.save(os.path.join(OUTPUT_DIR, \"f10_shap_values.npy\"), shap_values)\n",
    "    print(f\"\\n SHAP VALUES SAUVEGARDÉES: f10_shap_values.npy\")\n",
    "    \n",
    "    # Recommandations finales\n",
    "    print(f\"\\n CONFORMITÉ PRODUCTION:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"    RGPD Article 22: Droit à l'explication RESPECTÉ\")\n",
    "    print(\"    Fair Lending: Explicabilité des décisions ASSURÉE\")\n",
    "    print(\"    Audit: Traçabilité complète DISPONIBLE\")\n",
    "    print(\"\\n    NEXT STEPS:\")\n",
    "    print(\"      1. Intégrer SHAP dans API de scoring\")\n",
    "    print(\"      2. Créer templates d'explication client\")\n",
    "    print(\"      3. Former équipe service client sur interprétation\")\n",
    "    print(\"      4. Documenter processus pour audit\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" ANALYSE SHAP TERMINÉE - MODÈLE PRODUCTION-READY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "else:\n",
    "    print(\"\\n ERREUR: Modèle non disponible\")\n",
    "    print(\"   Solution: Ré-exécuter les cellules de la Section E\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
